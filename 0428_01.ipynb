{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fe5e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecospold import parse_file_v2\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "# 全局 activity_id 到 (shortname, activityName) 的映射字典\n",
    "global_activity_mapping = {}\n",
    "\n",
    "# 初始化统计变量\n",
    "total_files = 0\n",
    "converted_files = 0\n",
    "failed_files = 0\n",
    "failed_file_names = []\n",
    "\n",
    "# 用于跟踪每个文件中被删除的行数\n",
    "deleted_rows_per_file = {}\n",
    "\n",
    "# 用于跟踪每个文件中被替换的 process_name\n",
    "replaced_process_name = {}\n",
    "\n",
    "# 用于统计没有 activityLinkId 且 amount 不为1的行数\n",
    "non1_amount_per_file = {}\n",
    "\n",
    "# 用于统计没有 activityLinkId 且 amount 为-1的行数\n",
    "neg1_amount_per_file = {}\n",
    "\n",
    "# 用于记录仍有 [Unknown Location]Unknown Activity Name 的文件\n",
    "unknown_activity_files = []\n",
    "\n",
    "# 定义输入和输出文件夹路径\n",
    "input_folder = \"C:\\\\Users\\\\WasteWang\\\\LCA\\\\DATA\\\\3.11_APOS\\\\datasets\"\n",
    "output_folder_base = \"C:\\\\Users\\\\WasteWang\\\\LCA\\\\OUTPUT\"\n",
    "lookup_file_path = \"C:\\\\Users\\\\WasteWang\\\\LCA\\\\DATA\\\\3.11_APOS\\\\FilenameToActivityLookup.csv\"\n",
    "batch_file_path = \"C:\\\\Users\\\\WasteWang\\\\LCA\\\\batch_number.txt\"  # 用于存储批次编号的文件路径\n",
    "\n",
    "# 如果没有批次编号文件，初始化为 1\n",
    "if not os.path.exists(batch_file_path):\n",
    "    try:\n",
    "        with open(batch_file_path, \"w\") as f:\n",
    "            f.write(\"1\")\n",
    "        print(\"Batch number file created with initial value 1.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating batch number file: {e}\")\n",
    "\n",
    "# 读取批次编号\n",
    "try:\n",
    "    with open(batch_file_path, \"r\") as f:\n",
    "        batch_number = int(f.read().strip())\n",
    "    print(f\"Current batch number: {batch_number}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading batch number file: {e}\")\n",
    "    batch_number = 1  # 默认值\n",
    "\n",
    "# 获取当前日期\n",
    "current_date = datetime.now().strftime(\"%m%d\")\n",
    "print(f\"Current date: {current_date}\")\n",
    "\n",
    "# 创建当前批次的输出文件夹\n",
    "batch_folder = os.path.join(output_folder_base, f\"{current_date}_{batch_number}\")\n",
    "os.makedirs(batch_folder, exist_ok=True)\n",
    "print(f\"Output will be saved to: {batch_folder}\")\n",
    "\n",
    "# 配置日志，仅输出到文件，设置为 INFO 级别\n",
    "logger = logging.getLogger('spold_processor')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# 创建文件处理器，保存到输出批次文件夹中的 processing_debug.txt\n",
    "file_handler = logging.FileHandler(os.path.join(batch_folder, 'processing_debug.txt'), encoding='utf-8')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s:%(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# 加载文件名到 activity 名称的映射表，指定分号分隔符\n",
    "try:\n",
    "    lookup_df = pd.read_csv(lookup_file_path, sep=';')\n",
    "    lookup_dict = {row['Filename'].split('_')[0]: (row['ActivityName'], row['Location']) for _, row in lookup_df.iterrows()}\n",
    "    logger.info(f\"Loaded lookup dictionary with {len(lookup_dict)} entries.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading lookup file: {e}\")\n",
    "    lookup_dict = {}\n",
    "\n",
    "# 验证 lookup_dict 的完整性\n",
    "def verify_lookup_dict():\n",
    "    missing_prefixes = set()\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".spold\"):\n",
    "            prefix = filename.split('_')[0] if '_' in filename else os.path.splitext(filename)[0]\n",
    "            if prefix not in lookup_dict:\n",
    "                missing_prefixes.add(prefix)\n",
    "    if missing_prefixes:\n",
    "        logger.warning(f\"The following prefixes are missing in lookup_dict: {', '.join(missing_prefixes)}\")\n",
    "    else:\n",
    "        logger.info(\"All prefixes are present in lookup_dict.\")\n",
    "\n",
    "verify_lookup_dict()\n",
    "\n",
    "def extract_activity_mapping(file_path):\n",
    "    \"\"\"\n",
    "    提取单个 .spold 文件中的所有 activityDescription 元素，并返回一个 mapping 字典。\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    try:\n",
    "        logger.info(f\"Parsing file: {file_path}\")\n",
    "        ecoSpold = parse_file_v2(file_path)\n",
    "        namespaces = {'eco': 'http://www.EcoInvent.org/EcoSpold02'}\n",
    "        \n",
    "        # 获取所有 activityDescription 元素，包括子活动\n",
    "        activity_descriptions = ecoSpold.findall('.//eco:activityDescription', namespaces)\n",
    "        for activity_description in activity_descriptions:\n",
    "            activity = activity_description.find('eco:activity', namespaces)\n",
    "            if activity is not None:\n",
    "                activity_id = activity.attrib.get('id')\n",
    "                activity_name_elem = activity.find('eco:activityName', namespaces)\n",
    "                activity_name_text = activity_name_elem.text.strip() if activity_name_elem is not None and activity_name_elem.text else \"Unknown Activity Name\"\n",
    "\n",
    "                geography = activity_description.find('eco:geography', namespaces)\n",
    "                if geography is not None:\n",
    "                    shortname_elem = geography.find('eco:shortname', namespaces)\n",
    "                    shortname = shortname_elem.text.strip() if shortname_elem is not None and shortname_elem.text else \"Unknown Location\"\n",
    "                else:\n",
    "                    shortname = \"Unknown Location\"\n",
    "\n",
    "                if activity_id:\n",
    "                    mapping[activity_id] = (shortname, activity_name_text)\n",
    "                    logger.info(f\"Mapped activity_id {activity_id} to ({shortname}, {activity_name_text})\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting activity mapping from file {file_path}: {e}\")\n",
    "    return mapping\n",
    "\n",
    "def process_activity_description(activityDescription, current_activity_name, namespaces):\n",
    "    \"\"\"\n",
    "    处理 activityDescription 部分的信息，返回一个列表的字典记录。\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    try:\n",
    "        if activityDescription is not None:\n",
    "            for field, tag in [('includedActivitiesStart', 'includedActivitiesStart'),\n",
    "                               ('includedActivitiesEnd', 'includedActivitiesEnd'),\n",
    "                               ('generalComment', 'generalComment')]:\n",
    "                value = activityDescription.find(f'eco:{tag}', namespaces)\n",
    "                value_text = value.text.strip() if value is not None and value.text else 'N/A'\n",
    "                category = 'Included Activities' if field != 'generalComment' else 'General Comment'\n",
    "                record = {\n",
    "                    'process_name': current_activity_name,\n",
    "                    'flow': '',  # 该字段在此部分没有数据\n",
    "                    'unit': '',\n",
    "                    'amount': '',\n",
    "                    'category': category,\n",
    "                    'field': field,\n",
    "                    'value': value_text,\n",
    "                    'compartment': '',\n",
    "                    'subcompartment': '',\n",
    "                    'comment': '',\n",
    "                    'outputGroup': '',\n",
    "                    'section': 'activityDescription',\n",
    "                    'activityLinkId': '',\n",
    "                    'intermediateExchangeId': ''\n",
    "                }\n",
    "                records.append(record)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing activityDescription: {e}\")\n",
    "    return records\n",
    "\n",
    "def process_intermediate_exchange(exchange, current_activity_name, current_location, filename, file_activity_mapping, namespaces):\n",
    "    \"\"\"\n",
    "    处理一个 intermediateExchange 元素，返回一个字典记录。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        intermediateExchangeId = exchange.attrib.get('intermediateExchangeId', '').strip()\n",
    "        amount_str = exchange.attrib.get('amount', 'N/A').strip()\n",
    "        try:\n",
    "            amount = float(amount_str)\n",
    "        except ValueError:\n",
    "            amount = None\n",
    "            logger.warning(f\"{filename}: Invalid amount value '{amount_str}' in intermediateExchangeId '{intermediateExchangeId}'.\")\n",
    "\n",
    "        flow_name_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}name')\n",
    "        flow_name = flow_name_elem.text.strip() if flow_name_elem is not None and flow_name_elem.text else \"Unknown Flow\"\n",
    "        unit_name_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}unitName')\n",
    "        unit_name = unit_name_elem.text.strip() if unit_name_elem is not None and unit_name_elem.text else \"Unknown Unit\"\n",
    "        comment_field = exchange.find('{http://www.EcoInvent.org/EcoSpold02}comment')\n",
    "        comment = comment_field.text.strip() if comment_field is not None and comment_field.text else 'N/A'\n",
    "        outputGroup_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}outputGroup')\n",
    "        outputGroup = outputGroup_elem.text.strip() if outputGroup_elem is not None and outputGroup_elem.text else 'N/A'\n",
    "        activityLinkId = exchange.attrib.get('activityLinkId', '').strip()\n",
    "\n",
    "        # 条件：对于 intermediateExchange，如果 amount == 0，则删除该行\n",
    "        if amount == 0.0:\n",
    "            logger.info(f\"{filename}: Skipping intermediateExchange with amount=0.0.\")\n",
    "            return None  # 不添加该记录\n",
    "\n",
    "        # 使用 activityLinkId 获取对应的 shortname 和 activityName\n",
    "        if activityLinkId:\n",
    "            related_info = global_activity_mapping.get(activityLinkId)\n",
    "            if not related_info:\n",
    "                # 尝试从当前文件的 mapping 获取\n",
    "                related_info = file_activity_mapping.get(activityLinkId)\n",
    "            if not related_info:\n",
    "                related_info = (\"Unknown Location\", \"Unknown Activity Name\")\n",
    "                logger.warning(f\"{filename}: activityLinkId '{activityLinkId}' not found in global or file mapping.\")\n",
    "        else:\n",
    "            # 如果没有 activityLinkId，使用当前文件的 shortname 和 activityName\n",
    "            related_info = (current_location, current_activity_name)\n",
    "\n",
    "        related_shortname, related_activity_name = related_info\n",
    "\n",
    "        # 格式化 flow 字段\n",
    "        formatted_flow = f\"{flow_name}//[{related_shortname}]{related_activity_name}\"\n",
    "\n",
    "        # 创建记录字典\n",
    "        record = {\n",
    "            'process_name': current_activity_name,\n",
    "            'flow': formatted_flow,\n",
    "            'unit': unit_name,\n",
    "            'amount': amount,\n",
    "            'category': 'Flow Data',\n",
    "            'field': 'intermediateExchange',\n",
    "            'value': '',\n",
    "            'compartment': '',\n",
    "            'subcompartment': '',\n",
    "            'comment': comment,\n",
    "            'outputGroup': outputGroup,\n",
    "            'section': 'intermediateExchange',\n",
    "            'activityLinkId': activityLinkId,\n",
    "            'intermediateExchangeId': intermediateExchangeId\n",
    "        }\n",
    "        return record\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{filename}: Error processing intermediateExchange - {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_elementary_exchange(exchange, current_activity_name, filename, file_activity_mapping, namespaces):\n",
    "    \"\"\"\n",
    "    处理一个 elementaryExchange 元素，返回一个字典记录。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        flow_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}name')\n",
    "        flow = flow_elem.text.strip() if flow_elem is not None and flow_elem.text else \"Unknown Flow\"\n",
    "        unit_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}unitName')\n",
    "        unit = unit_elem.text.strip() if unit_elem is not None and unit_elem.text else \"Unknown Unit\"\n",
    "        amount_str = exchange.attrib.get('amount', 'N/A').strip()\n",
    "        try:\n",
    "            amount = float(amount_str)\n",
    "        except ValueError:\n",
    "            amount = None\n",
    "            logger.warning(f\"{filename}: Invalid amount value '{amount_str}' in elementaryExchangeId '{exchange.attrib.get('intermediateExchangeId', '').strip()}'.\")\n",
    "    \n",
    "        comment_field = exchange.find('{http://www.EcoInvent.org/EcoSpold02}comment')\n",
    "        comment = comment_field.text.strip() if comment_field is not None and comment_field.text else 'N/A'\n",
    "\n",
    "        # 条件：对于 elementaryExchange，如果 amount == 0，则删除该行\n",
    "        if amount == 0.0:\n",
    "            logger.info(f\"{filename}: Skipping elementaryExchange with amount=0.0.\")\n",
    "            return None  # 不添加该记录\n",
    "\n",
    "        # 处理 compartment 和 subcompartment\n",
    "        compartment = exchange.find('{http://www.EcoInvent.org/EcoSpold02}compartment')\n",
    "        if compartment is not None:\n",
    "            compartment_main = compartment.find('{http://www.EcoInvent.org/EcoSpold02}compartment')\n",
    "            compartment_text = compartment_main.text.strip() if compartment_main is not None and compartment_main.text else 'Unknown Compartment'\n",
    "            subcompartment_elem = compartment.find('{http://www.EcoInvent.org/EcoSpold02}subcompartment')\n",
    "            subcompartment = subcompartment_elem.text.strip() if subcompartment_elem is not None and subcompartment_elem.text else 'Unknown Subcompartment'\n",
    "        else:\n",
    "            compartment_text = 'Unknown Compartment'\n",
    "            subcompartment = 'Unknown Subcompartment'\n",
    "\n",
    "        outputGroup_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}outputGroup')\n",
    "        outputGroup = outputGroup_elem.text.strip() if outputGroup_elem is not None and outputGroup_elem.text else 'N/A'\n",
    "        activityLinkId = exchange.attrib.get('activityLinkId', '').strip()\n",
    "        intermediateExchangeId = exchange.attrib.get('intermediateExchangeId', '').strip()\n",
    "\n",
    "        # 使用 activityLinkId 获取对应的 shortname 和 activityName\n",
    "        if activityLinkId:\n",
    "            related_info = global_activity_mapping.get(activityLinkId)\n",
    "            if not related_info:\n",
    "                # 尝试从当前文件的 mapping 获取\n",
    "                related_info = file_activity_mapping.get(activityLinkId)\n",
    "            if not related_info:\n",
    "                related_info = (\"Unknown Location\", \"Unknown Activity Name\")\n",
    "                logger.warning(f\"{filename}: activityLinkId '{activityLinkId}' not found in global or file mapping.\")\n",
    "        else:\n",
    "            # 如果没有 activityLinkId，使用当前文件的 shortname 和 activityName\n",
    "            related_info = (current_activity_name, current_activity_name)  # 这里假设没有 activityLinkId 时使用当前活动信息\n",
    "\n",
    "        related_shortname, related_activity_name = related_info\n",
    "\n",
    "        # 格式化 flow 字段\n",
    "        formatted_flow = f\"{flow}//[{related_shortname}]{related_activity_name}\"\n",
    "\n",
    "        # 在 flow 中添加 compartment 和 subcompartment\n",
    "        formatted_flow = f\"{formatted_flow}_{compartment_text}_{subcompartment}\"\n",
    "\n",
    "        # 创建记录字典\n",
    "        record = {\n",
    "            'process_name': current_activity_name,\n",
    "            'flow': formatted_flow,\n",
    "            'unit': unit,\n",
    "            'amount': amount,\n",
    "            'category': 'Flow Data',\n",
    "            'field': 'elementaryExchange',\n",
    "            'value': '',\n",
    "            'compartment': compartment_text,\n",
    "            'subcompartment': subcompartment,\n",
    "            'comment': comment,\n",
    "            'outputGroup': outputGroup,\n",
    "            'section': 'elementaryExchange',\n",
    "            'activityLinkId': activityLinkId,\n",
    "            'intermediateExchangeId': intermediateExchangeId\n",
    "        }\n",
    "        return record\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{filename}: Error processing elementaryExchange - {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_file(filename):\n",
    "    global converted_files, failed_files, failed_file_names\n",
    "    global deleted_rows_per_file, replaced_process_name, non1_amount_per_file, neg1_amount_per_file, global_activity_mapping, unknown_activity_files\n",
    "\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    logger.info(f\"Processing file {filename}\")\n",
    "\n",
    "    try:\n",
    "        # 获取文件名前缀和后缀\n",
    "        if \"_\" in filename:\n",
    "            prefix, suffix_with_ext = filename.split(\"_\", 1)\n",
    "            suffix = os.path.splitext(suffix_with_ext)[0].strip()\n",
    "            logger.info(f\"{filename}: Filename split into prefix: '{prefix}' and suffix: '{suffix}'\")\n",
    "        else:\n",
    "            prefix = os.path.splitext(filename)[0].strip()\n",
    "            suffix = \"\"\n",
    "            logger.warning(f\"{filename}: Filename does not contain '_'. Using entire name as prefix.\")\n",
    "\n",
    "        # 获取 activityName 和 location\n",
    "        file_prefix = prefix  # 已经提取 prefix\n",
    "        process_info = lookup_dict.get(file_prefix, (\"Unknown Activity\", \"Unknown Location\"))\n",
    "        current_activity_name, current_location = process_info\n",
    "        logger.info(f\"{filename}: Current activity: '{current_activity_name}', Location: '{current_location}'\")\n",
    "\n",
    "        # 初始化一个空列表来存储所有记录\n",
    "        records = []\n",
    "\n",
    "        # 初始化删除计数\n",
    "        deleted_rows_per_file[filename] = 0\n",
    "\n",
    "        # 初始化非1的amount计数\n",
    "        non1_amount_per_file[filename] = 0\n",
    "\n",
    "        # 初始化-1的amount计数\n",
    "        neg1_amount_per_file[filename] = 0\n",
    "\n",
    "        # 解析 .spold 文件\n",
    "        ecoSpold = parse_file_v2(file_path)\n",
    "        namespaces = {'eco': 'http://www.EcoInvent.org/EcoSpold02'}\n",
    "\n",
    "        # 提取当前文件的 activity mapping\n",
    "        file_activity_mapping = extract_activity_mapping(file_path)\n",
    "\n",
    "        # 提取 activityDescription 部分的信息\n",
    "        activityDescription = ecoSpold.find('.//eco:activityDescription', namespaces)\n",
    "        activity_records = process_activity_description(activityDescription, current_activity_name, namespaces)\n",
    "        records.extend(activity_records)\n",
    "        logger.info(f\"{filename}: Extracted {len(activity_records)} activityDescription records.\")\n",
    "\n",
    "        # 提取 intermediateExchange 部分的信息\n",
    "        intermediate_exchanges = ecoSpold.findall('.//eco:intermediateExchange', namespaces)\n",
    "        logger.info(f\"{filename}: Found {len(intermediate_exchanges)} intermediateExchange elements.\")\n",
    "\n",
    "        # 识别所有与 suffix 匹配的 intermediateExchangeId\n",
    "        if suffix:\n",
    "            matching_exchanges = [exchange for exchange in intermediate_exchanges if exchange.attrib.get('intermediateExchangeId', '').strip() == suffix]\n",
    "            logger.info(f\"{filename}: Found {len(matching_exchanges)} intermediateExchange elements matching suffix '{suffix}'.\")\n",
    "        else:\n",
    "            matching_exchanges = []\n",
    "            logger.warning(f\"{filename}: Suffix is empty. No intermediateExchangeId to match.\")\n",
    "\n",
    "        # 按照逻辑更新 activityLinkId\n",
    "        if len(matching_exchanges) == 1:\n",
    "            exchange_to_update = matching_exchanges[0]\n",
    "            exchange_to_update.attrib['activityLinkId'] = prefix\n",
    "            logger.info(f\"{filename}: Only one matching intermediateExchangeId. Updated activityLinkId to '{prefix}'.\")\n",
    "        elif len(matching_exchanges) > 1:\n",
    "            # 更新所有 amount=1 或 amount=-1 的 activityLinkId\n",
    "            # 先查找所有 amount=1 或 1.0\n",
    "            amount_1_exchanges = [ex for ex in matching_exchanges if ex.attrib.get('amount', '').strip() in ['1', '1.0']]\n",
    "            if amount_1_exchanges:\n",
    "                for ex in amount_1_exchanges:\n",
    "                    ex.attrib['activityLinkId'] = prefix\n",
    "                    logger.info(f\"{filename}: Updated activityLinkId to '{prefix}' for intermediateExchangeId '{ex.attrib.get('intermediateExchangeId')}' with amount={ex.attrib.get('amount')}.\")\n",
    "            else:\n",
    "                # 如果没有 amount=1，则查找 amount=-1 或 -1.0\n",
    "                amount_neg1_exchanges = [ex for ex in matching_exchanges if ex.attrib.get('amount', '').strip() in ['-1', '-1.0']]\n",
    "                if amount_neg1_exchanges:\n",
    "                    for ex in amount_neg1_exchanges:\n",
    "                        ex.attrib['activityLinkId'] = prefix\n",
    "                        logger.info(f\"{filename}: Updated activityLinkId to '{prefix}' for intermediateExchangeId '{ex.attrib.get('intermediateExchangeId')}' with amount={ex.attrib.get('amount')}.\")\n",
    "                else:\n",
    "                    logger.info(f\"{filename}: Multiple matches. No amount=1 or amount=-1. No changes made.\")\n",
    "\n",
    "        # 处理所有 intermediateExchange\n",
    "        for exchange in intermediate_exchanges:\n",
    "            record = process_intermediate_exchange(exchange, current_activity_name, current_location, filename, file_activity_mapping, namespaces)\n",
    "            if record:\n",
    "                records.append(record)\n",
    "            else:\n",
    "                deleted_rows_per_file[filename] += 1\n",
    "\n",
    "            # 统计没有 activityLinkId 且 amount !=1 的情况\n",
    "            activityLinkId = exchange.attrib.get('activityLinkId', '').strip()\n",
    "            amount_str = exchange.attrib.get('amount', 'N/A').strip()\n",
    "            try:\n",
    "                amount = float(amount_str)\n",
    "            except ValueError:\n",
    "                amount = None\n",
    "                logger.warning(f\"{filename}: Invalid amount value '{amount_str}' in intermediateExchangeId '{exchange.attrib.get('intermediateExchangeId', '').strip()}'.\")\n",
    "        \n",
    "            if not activityLinkId and amount is not None and amount != 1.0:\n",
    "                non1_amount_per_file[filename] = non1_amount_per_file.get(filename, 0) + 1\n",
    "                logger.info(f\"{filename}: Found intermediateExchange with amount={amount} and no activityLinkId.\")\n",
    "                if amount == -1.0:\n",
    "                    neg1_amount_per_file[filename] = neg1_amount_per_file.get(filename, 0) + 1\n",
    "                    logger.info(f\"{filename}: Found intermediateExchange with amount=-1.0 and no activityLinkId.\")\n",
    "\n",
    "        # 提取 elementaryExchange 部分的信息\n",
    "        elementary_exchanges = ecoSpold.findall('.//eco:elementaryExchange', namespaces)\n",
    "        logger.info(f\"{filename}: Found {len(elementary_exchanges)} elementaryExchange elements.\")\n",
    "        for exchange in elementary_exchanges:\n",
    "            record = process_elementary_exchange(exchange, current_activity_name, filename, file_activity_mapping, namespaces)\n",
    "            if record:  # 只有在处理成功时才添加\n",
    "                records.append(record)\n",
    "            else:\n",
    "                deleted_rows_per_file[filename] += 1\n",
    "\n",
    "            # 统计没有 activityLinkId 且 amount !=1 的情况\n",
    "            activityLinkId = exchange.attrib.get('activityLinkId', '').strip()\n",
    "            amount_str = exchange.attrib.get('amount', 'N/A').strip()\n",
    "            try:\n",
    "                amount = float(amount_str)\n",
    "            except ValueError:\n",
    "                amount = None\n",
    "                logger.warning(f\"{filename}: Invalid amount value '{amount_str}' in elementaryExchangeId '{exchange.attrib.get('intermediateExchangeId', '').strip()}'.\")\n",
    "    \n",
    "            if not activityLinkId and amount is not None and amount != 1.0:\n",
    "                non1_amount_per_file[filename] = non1_amount_per_file.get(filename, 0) + 1\n",
    "                logger.info(f\"{filename}: Found elementaryExchange with amount={amount} and no activityLinkId.\")\n",
    "                if amount == -1.0:\n",
    "                    neg1_amount_per_file[filename] = neg1_amount_per_file.get(filename, 0) + 1\n",
    "                    logger.info(f\"{filename}: Found elementaryExchange with amount=-1.0 and no activityLinkId.\")\n",
    "\n",
    "        # 创建 DataFrame\n",
    "        all_data = pd.DataFrame(records, columns=[\n",
    "            'process_name', 'flow', 'unit', 'amount', 'category', 'field', 'value',\n",
    "            'compartment', 'subcompartment', 'comment', 'outputGroup', 'section',\n",
    "            'activityLinkId', 'intermediateExchangeId'\n",
    "        ])\n",
    "\n",
    "        # 生成组合字符串并检查是否与 \"prefix_suffix\" 匹配\n",
    "        matched_flow = None\n",
    "        target_combined = f\"{prefix}_{suffix}\"\n",
    "        logger.info(f\"{filename}: Target combined string: '{target_combined}'\")\n",
    "\n",
    "        # 创建布尔条件\n",
    "        condition = (all_data['activityLinkId'].fillna('') + '_' + all_data['intermediateExchangeId'].fillna('')) == target_combined\n",
    "\n",
    "        if any(condition):\n",
    "            matched_flow = all_data.loc[condition, 'flow'].iloc[0]  # 假设只有一个匹配\n",
    "            logger.info(f\"{filename}: Matched combined string '{target_combined}'. Setting process_name to '{matched_flow}' for all records.\")\n",
    "\n",
    "        # 根据匹配结果，替换整个文件的 process_name\n",
    "        if matched_flow:\n",
    "            all_data['process_name'] = matched_flow  # 替换整个列\n",
    "            replaced_process_name[filename] = matched_flow\n",
    "            logger.info(f\"{filename}: process_name has been set to '{matched_flow}' for all records based on combination '{target_combined}'.\")\n",
    "\n",
    "        # 检查是否有 flow 字段包含 [Unknown Location]Unknown Activity Name\n",
    "        if any(all_data['flow'].str.contains(r'\\[Unknown Location\\]Unknown Activity Name')):\n",
    "            unknown_activity_files.append(filename)\n",
    "            logger.warning(f\"{filename}: Contains '[Unknown Location]Unknown Activity Name' in flow fields after processing.\")\n",
    "\n",
    "        # 仅选择 'Flow Data' 类别的记录，并创建副本\n",
    "        flow_data = all_data[all_data['category'] == 'Flow Data'].copy()\n",
    "        logger.info(f\"{filename}: 'Flow Data' records count: {len(flow_data)}\")\n",
    "\n",
    "        # **新增功能：修改 'flow' 列中符合条件的内容**\n",
    "        # 定义修改 flow 的函数\n",
    "        def modify_flow(flow):\n",
    "            \"\"\"\n",
    "            修改 flow 字段的内容：\n",
    "            如果 flow 包含 \"//\"，则删除从 \"//\" 开始到第一个 \"_\" 之前的所有内容，\n",
    "            并在保留部分之间添加一个 \"_\"\n",
    "            \"\"\"\n",
    "            if '//' in flow:\n",
    "                try:\n",
    "                    start = flow.index('//')\n",
    "                    # 查找从 start 开始后的第一个 '_'\n",
    "                    after_start = flow[start:].index('_')\n",
    "                    end = start + after_start + 1  # 包括 '_'\n",
    "                    # 删除从 \"//\" 到第一个 \"_\" 之前的内容，并添加 \"_\"\n",
    "                    modified_flow = flow[:start] + '_' + flow[end:]\n",
    "                    logger.debug(f\"Original flow: '{flow}' | Modified flow: '{modified_flow}'\")\n",
    "                    return modified_flow\n",
    "                except ValueError:\n",
    "                    # 如果没有找到 '_', 返回原始 flow\n",
    "                    logger.warning(f\"Flow '{flow}' contains '//' but no '_'. No modification applied.\")\n",
    "                    return flow\n",
    "            else:\n",
    "                return flow\n",
    "\n",
    "        # 应用 modify_flow 函数到 'flow' 列中符合条件的行\n",
    "        try:\n",
    "            flow_data.loc[flow_data['field'] == 'elementaryExchange', 'flow'] = flow_data.loc[flow_data['field'] == 'elementaryExchange', 'flow'].apply(modify_flow)\n",
    "            logger.info(f\"{filename}: Modified 'flow' column for 'elementaryExchange' records.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{filename}: Error modifying 'flow' column for 'elementaryExchange' - {e}\")\n",
    "\n",
    "        # **新增功能结束**\n",
    "\n",
    "        # **新增功能：删除 outputGroup 为 'N/A' 的重复行**\n",
    "        # 查找除了 'outputGroup' 外完全相同的行\n",
    "        subset_cols = [col for col in flow_data.columns if col != 'outputGroup']\n",
    "        duplicates = flow_data.duplicated(subset=subset_cols, keep=False)\n",
    "\n",
    "        duplicated_flow_data = flow_data[duplicates]\n",
    "\n",
    "        # 初始化列表保存要删除的索引\n",
    "        indexes_to_drop = []\n",
    "\n",
    "        # 分组查找重复行\n",
    "        grouped = duplicated_flow_data.groupby(subset_cols)\n",
    "\n",
    "        for group_keys, group in grouped:\n",
    "            if set(group['outputGroup']) == {'0', 'N/A'}:\n",
    "                # 找到 outputGroup 为 'N/A' 的行并标记为删除\n",
    "                n_a_rows = group[group['outputGroup'] == 'N/A']\n",
    "                indexes_to_drop.extend(n_a_rows.index.tolist())\n",
    "\n",
    "        if indexes_to_drop:\n",
    "            flow_data = flow_data.drop(indexes_to_drop)\n",
    "            logger.info(f\"{filename}: Removed {len(indexes_to_drop)} duplicate rows with outputGroup 'N/A'.\")\n",
    "\n",
    "        # **新增功能结束**\n",
    "\n",
    "        # 生成输出文件路径，使用 os.path.splitext 确保正确替换扩展名\n",
    "        output_filename = f\"{os.path.splitext(filename)[0]}.csv\"\n",
    "        output_path = os.path.join(batch_folder, output_filename)\n",
    "\n",
    "        # 保存 'Flow Data' 相关的记录到 CSV 文件\n",
    "        try:\n",
    "            # 即使 flow_data 为空，也生成一个包含表头的 CSV 文件\n",
    "            flow_data.to_csv(output_path, index=False, encoding='utf-8')\n",
    "            converted_files += 1\n",
    "            logger.info(f\"{filename}: Flow data processed and saved to '{output_path}'\")\n",
    "        except Exception as e:\n",
    "            failed_files += 1\n",
    "            failed_file_names.append(filename)\n",
    "            logger.error(f\"{filename}: Error saving CSV file - {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{filename}: Error processing file - {e}\")\n",
    "        failed_files += 1\n",
    "        failed_file_names.append(filename)\n",
    "\n",
    "def main():\n",
    "    global total_files, converted_files, failed_files, failed_file_names\n",
    "\n",
    "    # 第一遍遍历所有文件，构建全局映射\n",
    "    logger.info(\"Starting first pass to build global activity mapping.\")\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".spold\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            logger.info(f\"Building mapping from file {filename}\")\n",
    "            file_mapping = extract_activity_mapping(file_path)\n",
    "            global_activity_mapping.update(file_mapping)\n",
    "\n",
    "    logger.info(f\"Global activity mapping built with {len(global_activity_mapping)} entries.\")\n",
    "\n",
    "    # 输出全局映射字典到CSV文件以便检查\n",
    "    mapping_output_path = os.path.join(batch_folder, \"global_activity_mapping.csv\")\n",
    "    try:\n",
    "        with open(mapping_output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['activity_id', 'shortname', 'activityName'])\n",
    "            for activity_id, (shortname, activityName) in global_activity_mapping.items():\n",
    "                writer.writerow([activity_id, shortname, activityName])\n",
    "        logger.info(f\"Global activity mapping exported to {mapping_output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exporting global activity mapping to CSV: {e}\")\n",
    "\n",
    "    # 第二遍遍历所有文件，处理数据\n",
    "    logger.info(\"Starting second pass to process all files.\")\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".spold\"):\n",
    "            total_files += 1\n",
    "            logger.info(f\"Processing file {filename}\")\n",
    "\n",
    "            try:\n",
    "                process_file(filename)\n",
    "            except Exception as e:\n",
    "                failed_files += 1\n",
    "                failed_file_names.append(filename)\n",
    "                logger.error(f\"{filename}: Error processing file - {e}\")\n",
    "                continue\n",
    "\n",
    "    # 生成失败文件列表并保存到文件\n",
    "    failed_files_log_path = os.path.join(batch_folder, \"failed_files.txt\")\n",
    "    try:\n",
    "        with open(failed_files_log_path, 'w', encoding='utf-8') as f:\n",
    "            for fname in failed_file_names:\n",
    "                f.write(f\"{fname}\\n\")\n",
    "        logger.info(f\"Failed file names saved to {failed_files_log_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing failed files log: {e}\")\n",
    "\n",
    "    # 生成处理总结并保存到 summary.txt，包括删除的行数统计和 process_name 替换统计\n",
    "    summary_log_path = os.path.join(batch_folder, \"summary.txt\")\n",
    "    try:\n",
    "        with open(summary_log_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=== Processing Summary ===\\n\")\n",
    "            f.write(f\"Total .spold files found: {total_files}\\n\")\n",
    "            f.write(f\"Successfully converted to CSV: {converted_files}\\n\")\n",
    "            f.write(f\"Failed to convert: {failed_files}\\n\\n\")\n",
    "            if failed_files > 0:\n",
    "                f.write(\"List of failed files:\\n\")\n",
    "                for fname in failed_file_names:\n",
    "                    f.write(f\"- {fname}\\n\")\n",
    "            f.write(\"\\n=== Deleted Rows Per File ===\\n\")\n",
    "            for fname, count in deleted_rows_per_file.items():\n",
    "                f.write(f\"{fname}: {count} rows deleted\\n\")\n",
    "            f.write(\"\\n=== Replaced Process Name Per File ===\\n\")\n",
    "            for fname, flow in replaced_process_name.items():\n",
    "                f.write(f\"{fname}: process_name replaced with flow '{flow}'\\n\")\n",
    "            f.write(\"\\n=== Non-1 Amount Without activityLinkId Per File ===\\n\")\n",
    "            for fname, count in non1_amount_per_file.items():\n",
    "                f.write(f\"{fname}: {count} rows with amount != 1 and no activityLinkId\\n\")\n",
    "            f.write(\"\\n=== Amount=-1 Without activityLinkId Per File ===\\n\")\n",
    "            for fname, count in neg1_amount_per_file.items():\n",
    "                f.write(f\"{fname}: {count} rows with amount=-1 and no activityLinkId\\n\")\n",
    "            f.write(\"\\n=== Files with Unknown Activity Name After Deletion ===\\n\")\n",
    "            for fname in unknown_activity_files:\n",
    "                f.write(f\"{fname}\\n\")\n",
    "        logger.info(f\"Processing summary saved to {summary_log_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing summary log: {e}\")\n",
    "\n",
    "    # 保存 non1_amount_per_file 到 CSV\n",
    "    output_non1_path = os.path.join(batch_folder, \"non1_amount_files.csv\")\n",
    "    try:\n",
    "        with open(output_non1_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['Filename', 'Non-1 Amount Count'])\n",
    "            for fname, count in non1_amount_per_file.items():\n",
    "                if count > 0:\n",
    "                    writer.writerow([fname, count])\n",
    "        logger.info(f\"Non-1 amount files saved to {output_non1_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exporting non1 amount files to CSV: {e}\")\n",
    "\n",
    "    # 保存 neg1_amount_per_file 到 CSV\n",
    "    output_neg1_path = os.path.join(batch_folder, \"neg1_amount_files.csv\")\n",
    "    try:\n",
    "        with open(output_neg1_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['Filename', 'Amount=-1 Count'])\n",
    "            for fname, count in neg1_amount_per_file.items():\n",
    "                if count > 0:\n",
    "                    writer.writerow([fname, count])\n",
    "        logger.info(f\"Amount=-1 files saved to {output_neg1_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exporting amount=-1 files to CSV: {e}\")\n",
    "\n",
    "    # 统计结果并记录到 summary.txt\n",
    "    logger.info(\"=== Processing Summary ===\")\n",
    "    logger.info(f\"Total .spold files found: {total_files}\")\n",
    "    logger.info(f\"Successfully converted to CSV: {converted_files}\")\n",
    "    logger.info(f\"Failed to convert: {failed_files}\")\n",
    "\n",
    "    if failed_files > 0:\n",
    "        logger.info(\"List of failed files:\")\n",
    "        for fname in failed_file_names:\n",
    "            logger.info(f\"- {fname}\")\n",
    "\n",
    "    # 记录每个文件被删除的行数\n",
    "    logger.info(\"=== Deleted Rows Per File ===\")\n",
    "    for fname, count in deleted_rows_per_file.items():\n",
    "        logger.info(f\"{fname}: {count} rows deleted\")\n",
    "\n",
    "    # 记录每个文件被替换的 process_name\n",
    "    logger.info(\"=== Replaced Process Name Per File ===\")\n",
    "    for fname, flow in replaced_process_name.items():\n",
    "        logger.info(f\"{fname}: process_name replaced with flow '{flow}'\")\n",
    "\n",
    "    # 记录没有 activityLinkId 且 amount !=1 的行数\n",
    "    logger.info(\"=== Non-1 Amount Without activityLinkId Per File ===\")\n",
    "    for fname, count in non1_amount_per_file.items():\n",
    "        logger.info(f\"{fname}: {count} rows with amount != 1 and no activityLinkId\")\n",
    "\n",
    "    # 记录没有 activityLinkId 且 amount ==-1 的行数\n",
    "    logger.info(\"=== Amount=-1 Without activityLinkId Per File ===\")\n",
    "    for fname, count in neg1_amount_per_file.items():\n",
    "        logger.info(f\"{fname}: {count} rows with amount=-1 and no activityLinkId\")\n",
    "\n",
    "    # 记录仍有 [Unknown Location]Unknown Activity Name 的文件\n",
    "    logger.info(\"=== Files with Unknown Activity Name After Deletion ===\")\n",
    "    for fname in unknown_activity_files:\n",
    "        logger.info(f\"{fname}\")\n",
    "\n",
    "    # 增加批次编号以便下一次运行\n",
    "    try:\n",
    "        with open(batch_file_path, \"w\") as f:\n",
    "            f.write(str(batch_number + 1))\n",
    "        logger.info(f\"Batch number incremented to {batch_number + 1}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating batch number: {e}\")\n",
    "\n",
    "    logger.info(\"All files processed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e44b4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batch number: 97\n",
      "Current date: 0428\n",
      "Output will be saved to: C:\\Users\\WasteWang\\LCA\\OUTPUT\\0428_97\n"
     ]
    }
   ],
   "source": [
    "from pyecospold import parse_file_v2\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "# 全局 activity_id 到 (shortname, activityName) 的映射字典\n",
    "global_activity_mapping = {}\n",
    "\n",
    "# 初始化统计变量\n",
    "total_files = 0\n",
    "converted_files = 0\n",
    "failed_files = 0\n",
    "failed_file_names = []\n",
    "\n",
    "# 用于跟踪每个文件中被删除的行数\n",
    "deleted_rows_per_file = {}\n",
    "\n",
    "# 用于跟踪每个文件中被替换的 process_name\n",
    "replaced_process_name = {}\n",
    "\n",
    "# 用于统计没有 activityLinkId 且 amount 不为1的行数\n",
    "non1_amount_per_file = {}\n",
    "\n",
    "# 用于统计没有 activityLinkId 且 amount 为-1的行数\n",
    "neg1_amount_per_file = {}\n",
    "\n",
    "# 用于记录仍有 [Unknown Location]Unknown Activity Name 的文件\n",
    "unknown_activity_files = []\n",
    "\n",
    "# 定义输入和输出文件夹路径\n",
    "input_folder = \"C:\\\\Users\\\\WasteWang\\\\LCA\\\\DATA\\\\3.11_APOS\\\\datasets\"\n",
    "output_folder_base = \"C:\\\\Users\\\\WasteWang\\\\LCA\\\\OUTPUT\"\n",
    "lookup_file_path = \"C:\\\\Users\\\\WasteWang\\\\LCA\\\\DATA\\\\3.11_APOS\\\\FilenameToActivityLookup.csv\"\n",
    "batch_file_path = \"C:\\\\Users\\\\WasteWang\\\\LCA\\\\batch_number.txt\"  # 用于存储批次编号的文件路径\n",
    "\n",
    "# 如果没有批次编号文件，初始化为 1\n",
    "if not os.path.exists(batch_file_path):\n",
    "    try:\n",
    "        with open(batch_file_path, \"w\") as f:\n",
    "            f.write(\"1\")\n",
    "        print(\"Batch number file created with initial value 1.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating batch number file: {e}\")\n",
    "\n",
    "# 读取批次编号\n",
    "try:\n",
    "    with open(batch_file_path, \"r\") as f:\n",
    "        batch_number = int(f.read().strip())\n",
    "    print(f\"Current batch number: {batch_number}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading batch number file: {e}\")\n",
    "    batch_number = 1  # 默认值\n",
    "\n",
    "# 获取当前日期\n",
    "current_date = datetime.now().strftime(\"%m%d\")\n",
    "print(f\"Current date: {current_date}\")\n",
    "\n",
    "# 创建当前批次的输出文件夹\n",
    "batch_folder = os.path.join(output_folder_base, f\"{current_date}_{batch_number}\")\n",
    "os.makedirs(batch_folder, exist_ok=True)\n",
    "print(f\"Output will be saved to: {batch_folder}\")\n",
    "\n",
    "# 配置日志，仅输出到文件，设置为 INFO 级别\n",
    "logger = logging.getLogger('spold_processor')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# 创建文件处理器，保存到输出批次文件夹中的 processing_debug.txt\n",
    "file_handler = logging.FileHandler(os.path.join(batch_folder, 'processing_debug.txt'), encoding='utf-8')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s:%(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# 加载文件名到 activity 名称的映射表，指定分号分隔符\n",
    "try:\n",
    "    lookup_df = pd.read_csv(lookup_file_path, sep=';')\n",
    "    lookup_dict = {row['Filename'].split('_')[0]: (row['ActivityName'], row['Location']) for _, row in lookup_df.iterrows()}\n",
    "    logger.info(f\"Loaded lookup dictionary with {len(lookup_dict)} entries.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading lookup file: {e}\")\n",
    "    lookup_dict = {}\n",
    "\n",
    "# 验证 lookup_dict 的完整性\n",
    "def verify_lookup_dict():\n",
    "    missing_prefixes = set()\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".spold\"):\n",
    "            prefix = filename.split('_')[0] if '_' in filename else os.path.splitext(filename)[0]\n",
    "            if prefix not in lookup_dict:\n",
    "                missing_prefixes.add(prefix)\n",
    "    if missing_prefixes:\n",
    "        logger.warning(f\"The following prefixes are missing in lookup_dict: {', '.join(missing_prefixes)}\")\n",
    "    else:\n",
    "        logger.info(\"All prefixes are present in lookup_dict.\")\n",
    "\n",
    "verify_lookup_dict()\n",
    "\n",
    "def extract_activity_mapping(file_path):\n",
    "    \"\"\"\n",
    "    提取单个 .spold 文件中的所有 activityDescription 元素，并返回一个 mapping 字典。\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    try:\n",
    "        logger.info(f\"Parsing file: {file_path}\")\n",
    "        ecoSpold = parse_file_v2(file_path)\n",
    "        namespaces = {'eco': 'http://www.EcoInvent.org/EcoSpold02'}\n",
    "        \n",
    "        # 获取所有 activityDescription 元素，包括子活动\n",
    "        activity_descriptions = ecoSpold.findall('.//eco:activityDescription', namespaces)\n",
    "        for activity_description in activity_descriptions:\n",
    "            activity = activity_description.find('eco:activity', namespaces)\n",
    "            if activity is not None:\n",
    "                activity_id = activity.attrib.get('id')\n",
    "                activity_name_elem = activity.find('eco:activityName', namespaces)\n",
    "                activity_name_text = activity_name_elem.text.strip() if activity_name_elem is not None and activity_name_elem.text else \"Unknown Activity Name\"\n",
    "\n",
    "                geography = activity_description.find('eco:geography', namespaces)\n",
    "                if geography is not None:\n",
    "                    shortname_elem = geography.find('eco:shortname', namespaces)\n",
    "                    shortname = shortname_elem.text.strip() if shortname_elem is not None and shortname_elem.text else \"Unknown Location\"\n",
    "                else:\n",
    "                    shortname = \"Unknown Location\"\n",
    "\n",
    "                if activity_id:\n",
    "                    mapping[activity_id] = (shortname, activity_name_text)\n",
    "                    logger.info(f\"Mapped activity_id {activity_id} to ({shortname}, {activity_name_text})\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting activity mapping from file {file_path}: {e}\")\n",
    "    return mapping\n",
    "\n",
    "def process_activity_description(activityDescription, current_activity_name, namespaces):\n",
    "    \"\"\"\n",
    "    处理 activityDescription 部分的信息，返回一个列表的字典记录。\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    try:\n",
    "        if activityDescription is not None:\n",
    "            for field, tag in [('includedActivitiesStart', 'includedActivitiesStart'),\n",
    "                               ('includedActivitiesEnd', 'includedActivitiesEnd'),\n",
    "                               ('generalComment', 'generalComment')]:\n",
    "                value = activityDescription.find(f'eco:{tag}', namespaces)\n",
    "                value_text = value.text.strip() if value is not None and value.text else 'N/A'\n",
    "                category = 'Included Activities' if field != 'generalComment' else 'General Comment'\n",
    "                record = {\n",
    "                    'process_name': current_activity_name,\n",
    "                    'flow': '',  # 该字段在此部分没有数据\n",
    "                    'unit': '',\n",
    "                    'amount': '',\n",
    "                    'category': category,\n",
    "                    'field': field,\n",
    "                    'value': value_text,\n",
    "                    'compartment': '',\n",
    "                    'subcompartment': '',\n",
    "                    'comment': '',\n",
    "                    'outputGroup': '',\n",
    "                    'section': 'activityDescription',\n",
    "                    'activityLinkId': '',\n",
    "                    'intermediateExchangeId': ''\n",
    "                }\n",
    "                records.append(record)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing activityDescription: {e}\")\n",
    "    return records\n",
    "\n",
    "def process_intermediate_exchange(exchange, current_activity_name, current_location, filename, file_activity_mapping, namespaces):\n",
    "    \"\"\"\n",
    "    处理一个 intermediateExchange 元素，返回一个字典记录。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        intermediateExchangeId = exchange.attrib.get('intermediateExchangeId', '').strip()\n",
    "        amount_str = exchange.attrib.get('amount', 'N/A').strip()\n",
    "        try:\n",
    "            amount = float(amount_str)\n",
    "        except ValueError:\n",
    "            amount = None\n",
    "            logger.warning(f\"{filename}: Invalid amount value '{amount_str}' in intermediateExchangeId '{intermediateExchangeId}'.\")\n",
    "        \n",
    "        flow_name_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}name')\n",
    "        flow_name = flow_name_elem.text.strip() if flow_name_elem is not None and flow_name_elem.text else \"Unknown Flow\"\n",
    "        unit_name_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}unitName')\n",
    "        unit_name = unit_name_elem.text.strip() if unit_name_elem is not None and unit_name_elem.text else \"Unknown Unit\"\n",
    "        comment_field = exchange.find('{http://www.EcoInvent.org/EcoSpold02}comment')\n",
    "        comment = comment_field.text.strip() if comment_field is not None and comment_field.text else 'N/A'\n",
    "        outputGroup_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}outputGroup')\n",
    "        outputGroup = outputGroup_elem.text.strip() if outputGroup_elem is not None and outputGroup_elem.text else 'N/A'\n",
    "        activityLinkId = exchange.attrib.get('activityLinkId', '').strip()\n",
    "\n",
    "        # 条件：对于 intermediateExchange，如果 amount == 0，则删除该行\n",
    "        if amount == 0.0:\n",
    "            logger.info(f\"{filename}: Skipping intermediateExchange with amount=0.0.\")\n",
    "            return None  # 不添加该记录\n",
    "\n",
    "        # 使用 activityLinkId 获取对应的 shortname 和 activityName\n",
    "        if activityLinkId:\n",
    "            related_info = global_activity_mapping.get(activityLinkId)\n",
    "            if not related_info:\n",
    "                # 尝试从当前文件的 mapping 获取\n",
    "                related_info = file_activity_mapping.get(activityLinkId)\n",
    "            if not related_info:\n",
    "                related_info = (\"Unknown Location\", \"Unknown Activity Name\")\n",
    "                logger.warning(f\"{filename}: activityLinkId '{activityLinkId}' not found in global or file mapping.\")\n",
    "        else:\n",
    "            # 如果没有 activityLinkId，使用当前文件的 shortname 和 activityName\n",
    "            related_info = (current_location, current_activity_name)\n",
    "\n",
    "        related_shortname, related_activity_name = related_info\n",
    "\n",
    "        # 格式化 flow 字段\n",
    "        formatted_flow = f\"{flow_name}//[{related_shortname}]{related_activity_name}\"\n",
    "\n",
    "        # 创建记录字典\n",
    "        record = {\n",
    "            'process_name': current_activity_name,\n",
    "            'flow': formatted_flow,\n",
    "            'unit': unit_name,\n",
    "            'amount': amount,\n",
    "            'category': 'Flow Data',\n",
    "            'field': 'intermediateExchange',\n",
    "            'value': '',\n",
    "            'compartment': '',\n",
    "            'subcompartment': '',\n",
    "            'comment': comment,\n",
    "            'outputGroup': outputGroup,\n",
    "            'section': 'intermediateExchange',\n",
    "            'activityLinkId': activityLinkId,\n",
    "            'intermediateExchangeId': intermediateExchangeId\n",
    "        }\n",
    "        return record\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{filename}: Error processing intermediateExchange - {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_elementary_exchange(exchange, current_activity_name, filename, file_activity_mapping, namespaces):\n",
    "    \"\"\"\n",
    "    处理一个 elementaryExchange 元素，返回一个字典记录。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        flow_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}name')\n",
    "        flow = flow_elem.text.strip() if flow_elem is not None and flow_elem.text else \"Unknown Flow\"\n",
    "        unit_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}unitName')\n",
    "        unit = unit_elem.text.strip() if unit_elem is not None and unit_elem.text else \"Unknown Unit\"\n",
    "        amount_str = exchange.attrib.get('amount', 'N/A').strip()\n",
    "        try:\n",
    "            amount = float(amount_str)\n",
    "        except ValueError:\n",
    "            amount = None\n",
    "            logger.warning(f\"{filename}: Invalid amount value '{amount_str}' in elementaryExchangeId '{exchange.attrib.get('intermediateExchangeId', '').strip()}'.\")\n",
    "    \n",
    "        comment_field = exchange.find('{http://www.EcoInvent.org/EcoSpold02}comment')\n",
    "        comment = comment_field.text.strip() if comment_field is not None and comment_field.text else 'N/A'\n",
    "\n",
    "        # 条件：对于 elementaryExchange，如果 amount == 0，则删除该行\n",
    "        if amount == 0.0:\n",
    "            logger.info(f\"{filename}: Skipping elementaryExchange with amount=0.0.\")\n",
    "            return None  # 不添加该记录\n",
    "\n",
    "        # 处理 compartment 和 subcompartment\n",
    "        compartment = exchange.find('{http://www.EcoInvent.org/EcoSpold02}compartment')\n",
    "        if compartment is not None:\n",
    "            compartment_main = compartment.find('{http://www.EcoInvent.org/EcoSpold02}compartment')\n",
    "            compartment_text = compartment_main.text.strip() if compartment_main is not None and compartment_main.text else 'Unknown Compartment'\n",
    "            subcompartment_elem = compartment.find('{http://www.EcoInvent.org/EcoSpold02}subcompartment')\n",
    "            subcompartment = subcompartment_elem.text.strip() if subcompartment_elem is not None and subcompartment_elem.text else 'Unknown Subcompartment'\n",
    "        else:\n",
    "            compartment_text = 'Unknown Compartment'\n",
    "            subcompartment = 'Unknown Subcompartment'\n",
    "\n",
    "        outputGroup_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}outputGroup')\n",
    "        outputGroup = outputGroup_elem.text.strip() if outputGroup_elem is not None and outputGroup_elem.text else 'N/A'\n",
    "        activityLinkId = exchange.attrib.get('activityLinkId', '').strip()\n",
    "        intermediateExchangeId = exchange.attrib.get('intermediateExchangeId', '').strip()\n",
    "\n",
    "        # 使用 activityLinkId 获取对应的 shortname 和 activityName\n",
    "        if activityLinkId:\n",
    "            related_info = global_activity_mapping.get(activityLinkId)\n",
    "            if not related_info:\n",
    "                # 尝试从当前文件的 mapping 获取\n",
    "                related_info = file_activity_mapping.get(activityLinkId)\n",
    "            if not related_info:\n",
    "                related_info = (\"Unknown Location\", \"Unknown Activity Name\")\n",
    "                logger.warning(f\"{filename}: activityLinkId '{activityLinkId}' not found in global or file mapping.\")\n",
    "        else:\n",
    "            # 如果没有 activityLinkId，使用当前文件的 shortname 和 activityName\n",
    "            related_info = (current_activity_name, current_activity_name)  # 这里假设没有 activityLinkId 时使用当前活动信息\n",
    "\n",
    "        related_shortname, related_activity_name = related_info\n",
    "\n",
    "        # 格式化 flow 字段\n",
    "        formatted_flow = f\"{flow}//[{related_shortname}]{related_activity_name}\"\n",
    "\n",
    "        # 在 flow 中添加 compartment 和 subcompartment\n",
    "        formatted_flow = f\"{formatted_flow}_{compartment_text}_{subcompartment}\"\n",
    "\n",
    "        # 创建记录字典\n",
    "        record = {\n",
    "            'process_name': current_activity_name,\n",
    "            'flow': formatted_flow,\n",
    "            'unit': unit,\n",
    "            'amount': amount,\n",
    "            'category': 'Flow Data',\n",
    "            'field': 'elementaryExchange',\n",
    "            'value': '',\n",
    "            'compartment': compartment_text,\n",
    "            'subcompartment': subcompartment,\n",
    "            'comment': comment,\n",
    "            'outputGroup': outputGroup,\n",
    "            'section': 'elementaryExchange',\n",
    "            'activityLinkId': activityLinkId,\n",
    "            'intermediateExchangeId': intermediateExchangeId\n",
    "        }\n",
    "        return record\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{filename}: Error processing elementaryExchange - {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_file(filename):\n",
    "    global converted_files, failed_files, failed_file_names\n",
    "    global deleted_rows_per_file, replaced_process_name, non1_amount_per_file, neg1_amount_per_file, global_activity_mapping, unknown_activity_files\n",
    "\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    logger.info(f\"Processing file {filename}\")\n",
    "\n",
    "    try:\n",
    "        # 获取文件名前缀和后缀\n",
    "        if \"_\" in filename:\n",
    "            prefix, suffix_with_ext = filename.split(\"_\", 1)\n",
    "            suffix = os.path.splitext(suffix_with_ext)[0].strip()\n",
    "            logger.info(f\"{filename}: Filename split into prefix: '{prefix}' and suffix: '{suffix}'\")\n",
    "        else:\n",
    "            prefix = os.path.splitext(filename)[0].strip()\n",
    "            suffix = \"\"\n",
    "            logger.warning(f\"{filename}: Filename does not contain '_'. Using entire name as prefix.\")\n",
    "\n",
    "        # 获取 activityName 和 location\n",
    "        file_prefix = prefix  # 已经提取 prefix\n",
    "        process_info = lookup_dict.get(file_prefix, (\"Unknown Activity\", \"Unknown Location\"))\n",
    "        current_activity_name, current_location = process_info\n",
    "        logger.info(f\"{filename}: Current activity: '{current_activity_name}', Location: '{current_location}'\")\n",
    "\n",
    "        # 初始化一个空列表来存储所有记录\n",
    "        records = []\n",
    "\n",
    "        # 初始化删除计数\n",
    "        deleted_rows_per_file[filename] = 0\n",
    "\n",
    "        # 初始化非1的amount计数\n",
    "        non1_amount_per_file[filename] = 0\n",
    "\n",
    "        # 初始化-1的amount计数\n",
    "        neg1_amount_per_file[filename] = 0\n",
    "\n",
    "        # 解析 .spold 文件\n",
    "        ecoSpold = parse_file_v2(file_path)\n",
    "        namespaces = {'eco': 'http://www.EcoInvent.org/EcoSpold02'}\n",
    "\n",
    "        # 提取当前文件的 activity mapping\n",
    "        file_activity_mapping = extract_activity_mapping(file_path)\n",
    "\n",
    "        # 提取 activityDescription 部分的信息\n",
    "        activityDescription = ecoSpold.find('.//eco:activityDescription', namespaces)\n",
    "        activity_records = process_activity_description(activityDescription, current_activity_name, namespaces)\n",
    "        records.extend(activity_records)\n",
    "        logger.info(f\"{filename}: Extracted {len(activity_records)} activityDescription records.\")\n",
    "\n",
    "        # 提取 intermediateExchange 部分的信息\n",
    "        intermediate_exchanges = ecoSpold.findall('.//eco:intermediateExchange', namespaces)\n",
    "        logger.info(f\"{filename}: Found {len(intermediate_exchanges)} intermediateExchange elements.\")\n",
    "\n",
    "        # 识别所有与 suffix 匹配的 intermediateExchangeId\n",
    "        if suffix:\n",
    "            matching_exchanges = [exchange for exchange in intermediate_exchanges if exchange.attrib.get('intermediateExchangeId', '').strip() == suffix]\n",
    "            logger.info(f\"{filename}: Found {len(matching_exchanges)} intermediateExchange elements matching suffix '{suffix}'.\")\n",
    "        else:\n",
    "            matching_exchanges = []\n",
    "            logger.warning(f\"{filename}: Suffix is empty. No intermediateExchangeId to match.\")\n",
    "\n",
    "        # 按照逻辑更新 activityLinkId\n",
    "        if len(matching_exchanges) == 1:\n",
    "            exchange_to_update = matching_exchanges[0]\n",
    "            exchange_to_update.attrib['activityLinkId'] = prefix\n",
    "            logger.info(f\"{filename}: Only one matching intermediateExchangeId. Updated activityLinkId to '{prefix}'.\")\n",
    "        elif len(matching_exchanges) > 1:\n",
    "            # 更新所有 amount=1 或 amount=-1 的 activityLinkId\n",
    "            # 先查找所有 amount=1 或 1.0\n",
    "            amount_1_exchanges = [ex for ex in matching_exchanges if ex.attrib.get('amount', '').strip() in ['1', '1.0']]\n",
    "            if amount_1_exchanges:\n",
    "                for ex in amount_1_exchanges:\n",
    "                    ex.attrib['activityLinkId'] = prefix\n",
    "                    logger.info(f\"{filename}: Updated activityLinkId to '{prefix}' for intermediateExchangeId '{ex.attrib.get('intermediateExchangeId')}' with amount={ex.attrib.get('amount')}.\")\n",
    "            else:\n",
    "                # 如果没有 amount=1，则查找 amount=-1 或 -1.0\n",
    "                amount_neg1_exchanges = [ex for ex in matching_exchanges if ex.attrib.get('amount', '').strip() in ['-1', '-1.0']]\n",
    "                if amount_neg1_exchanges:\n",
    "                    for ex in amount_neg1_exchanges:\n",
    "                        ex.attrib['activityLinkId'] = prefix\n",
    "                        logger.info(f\"{filename}: Updated activityLinkId to '{prefix}' for intermediateExchangeId '{ex.attrib.get('intermediateExchangeId')}' with amount={ex.attrib.get('amount')}.\")\n",
    "                else:\n",
    "                    logger.info(f\"{filename}: Multiple matches. No amount=1 or amount=-1. No changes made.\")\n",
    "\n",
    "        # 处理所有 intermediateExchange\n",
    "        for exchange in intermediate_exchanges:\n",
    "            record = process_intermediate_exchange(exchange, current_activity_name, current_location, filename, file_activity_mapping, namespaces)\n",
    "            if record:\n",
    "                records.append(record)\n",
    "            else:\n",
    "                deleted_rows_per_file[filename] += 1\n",
    "\n",
    "            # 统计没有 activityLinkId 且 amount !=1 的情况\n",
    "            activityLinkId = exchange.attrib.get('activityLinkId', '').strip()\n",
    "            amount_str = exchange.attrib.get('amount', 'N/A').strip()\n",
    "            try:\n",
    "                amount = float(amount_str)\n",
    "            except ValueError:\n",
    "                amount = None\n",
    "                logger.warning(f\"{filename}: Invalid amount value '{amount_str}' in intermediateExchangeId '{exchange.attrib.get('intermediateExchangeId', '').strip()}'.\")\n",
    "        \n",
    "            if not activityLinkId and amount is not None and amount != 1.0:\n",
    "                non1_amount_per_file[filename] = non1_amount_per_file.get(filename, 0) + 1\n",
    "                logger.info(f\"{filename}: Found intermediateExchange with amount={amount} and no activityLinkId.\")\n",
    "                if amount == -1.0:\n",
    "                    neg1_amount_per_file[filename] = neg1_amount_per_file.get(filename, 0) + 1\n",
    "                    logger.info(f\"{filename}: Found intermediateExchange with amount=-1.0 and no activityLinkId.\")\n",
    "\n",
    "        # 提取 elementaryExchange 部分的信息\n",
    "        elementary_exchanges = ecoSpold.findall('.//eco:elementaryExchange', namespaces)\n",
    "        logger.info(f\"{filename}: Found {len(elementary_exchanges)} elementaryExchange elements.\")\n",
    "        for exchange in elementary_exchanges:\n",
    "            record = process_elementary_exchange(exchange, current_activity_name, filename, file_activity_mapping, namespaces)\n",
    "            if record:  # 只有在处理成功时才添加\n",
    "                records.append(record)\n",
    "            else:\n",
    "                deleted_rows_per_file[filename] += 1\n",
    "\n",
    "            # 统计没有 activityLinkId 且 amount !=1 的情况\n",
    "            activityLinkId = exchange.attrib.get('activityLinkId', '').strip()\n",
    "            amount_str = exchange.attrib.get('amount', 'N/A').strip()\n",
    "            try:\n",
    "                amount = float(amount_str)\n",
    "            except ValueError:\n",
    "                amount = None\n",
    "                logger.warning(f\"{filename}: Invalid amount value '{amount_str}' in elementaryExchangeId '{exchange.attrib.get('intermediateExchangeId', '').strip()}'.\")\n",
    "    \n",
    "            if not activityLinkId and amount is not None and amount != 1.0:\n",
    "                non1_amount_per_file[filename] = non1_amount_per_file.get(filename, 0) + 1\n",
    "                logger.info(f\"{filename}: Found elementaryExchange with amount={amount} and no activityLinkId.\")\n",
    "                if amount == -1.0:\n",
    "                    neg1_amount_per_file[filename] = neg1_amount_per_file.get(filename, 0) + 1\n",
    "                    logger.info(f\"{filename}: Found elementaryExchange with amount=-1.0 and no activityLinkId.\")\n",
    "\n",
    "        # 创建 DataFrame\n",
    "        all_data = pd.DataFrame(records, columns=[\n",
    "            'process_name', 'flow', 'unit', 'amount', 'category', 'field', 'value',\n",
    "            'compartment', 'subcompartment', 'comment', 'outputGroup', 'section',\n",
    "            'activityLinkId', 'intermediateExchangeId'\n",
    "        ])\n",
    "\n",
    "        # 生成组合字符串并检查是否与 \"prefix_suffix\" 匹配\n",
    "        matched_flow = None\n",
    "        target_combined = f\"{prefix}_{suffix}\"\n",
    "        logger.info(f\"{filename}: Target combined string: '{target_combined}'\")\n",
    "\n",
    "        # 创建布尔条件\n",
    "        condition = (all_data['activityLinkId'].fillna('') + '_' + all_data['intermediateExchangeId'].fillna('')) == target_combined\n",
    "\n",
    "        if any(condition):\n",
    "            matched_flow = all_data.loc[condition, 'flow'].iloc[0]  # 假设只有一个匹配\n",
    "            logger.info(f\"{filename}: Matched combined string '{target_combined}'. Setting process_name to '{matched_flow}' for all records.\")\n",
    "\n",
    "        # 根据匹配结果，替换整个文件的 process_name\n",
    "        if matched_flow:\n",
    "            all_data['process_name'] = matched_flow  # 替换整个列\n",
    "            replaced_process_name[filename] = matched_flow\n",
    "            logger.info(f\"{filename}: process_name has been set to '{matched_flow}' for all records based on combination '{target_combined}'.\")\n",
    "\n",
    "        # 检查是否有 flow 字段包含 [Unknown Location]Unknown Activity Name\n",
    "        if any(all_data['flow'].str.contains(r'\\[Unknown Location\\]Unknown Activity Name', na=False)):\n",
    "            unknown_activity_files.append(filename)\n",
    "            logger.warning(f\"{filename}: Contains '[Unknown Location]Unknown Activity Name' in flow fields after processing.\")\n",
    "\n",
    "        # 仅选择 'Flow Data' 类别的记录，并创建副本\n",
    "        flow_data = all_data[all_data['category'] == 'Flow Data'].copy()\n",
    "        logger.info(f\"{filename}: 'Flow Data' records count: {len(flow_data)}\")\n",
    "\n",
    "        # **新增功能：修改 'flow' 列中符合条件的内容**\n",
    "        # 定义修改 flow 的函数\n",
    "        def modify_flow(flow):\n",
    "            \"\"\"\n",
    "            修改 flow 字段的内容：\n",
    "            如果 flow 包含 \"//\"，则删除从 \"//\" 开始到第一个 \"_\" 之前的所有内容，\n",
    "            并在保留部分之间添加一个 \"_\"\n",
    "            \"\"\"\n",
    "            if '//' in flow:\n",
    "                try:\n",
    "                    start = flow.index('//')\n",
    "                    # 查找从 start 开始后的第一个 '_'\n",
    "                    after_start = flow[start:].index('_')\n",
    "                    end = start + after_start + 1  # 包括 '_'\n",
    "                    # 删除从 \"//\" 到第一个 \"_\" 之前的内容，并添加 \"_\"\n",
    "                    modified_flow = flow[:start] + '_' + flow[end:]\n",
    "                    logger.debug(f\"Original flow: '{flow}' | Modified flow: '{modified_flow}'\")\n",
    "                    return modified_flow\n",
    "                except ValueError:\n",
    "                    # 如果没有找到 '_', 返回原始 flow\n",
    "                    logger.warning(f\"Flow '{flow}' contains '//' but no '_'. No modification applied.\")\n",
    "                    return flow\n",
    "            else:\n",
    "                return flow\n",
    "\n",
    "        # 应用 modify_flow 函数到 'flow' 列中符合条件的行\n",
    "        try:\n",
    "            # 仅对 'elementaryExchange' 的行应用修改\n",
    "            condition_modify = flow_data['field'] == 'elementaryExchange'\n",
    "            original_count = flow_data[condition_modify].shape[0]\n",
    "            flow_data.loc[condition_modify, 'flow'] = flow_data.loc[condition_modify, 'flow'].apply(modify_flow)\n",
    "            modified_count = flow_data[condition_modify]['flow'].str.contains('_').sum()  # 简单检查修改是否成功\n",
    "            logger.info(f\"{filename}: Modified 'flow' column for 'elementaryExchange' records. {modified_count}/{original_count} records modified.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{filename}: Error modifying 'flow' column for 'elementaryExchange' - {e}\")\n",
    "\n",
    "        # **新增功能结束**\n",
    "\n",
    "        # **新增功能：删除 outputGroup 为 'N/A' 的重复行**\n",
    "        # 查找除了 'outputGroup' 外完全相同的行\n",
    "        subset_cols = [col for col in flow_data.columns if col != 'outputGroup']\n",
    "        duplicates = flow_data.duplicated(subset=subset_cols, keep=False)\n",
    "\n",
    "        duplicated_flow_data = flow_data[duplicates]\n",
    "\n",
    "        # 初始化列表保存要删除的索引\n",
    "        indexes_to_drop = []\n",
    "\n",
    "        # 分组查找重复行\n",
    "        grouped = duplicated_flow_data.groupby(subset_cols)\n",
    "\n",
    "        for group_keys, group in grouped:\n",
    "            if set(group['outputGroup']) == {'0', 'N/A'}:\n",
    "                # 找到 outputGroup 为 'N/A' 的行并标记为删除\n",
    "                n_a_rows = group[group['outputGroup'] == 'N/A']\n",
    "                indexes_to_drop.extend(n_a_rows.index.tolist())\n",
    "\n",
    "        if indexes_to_drop:\n",
    "            flow_data = flow_data.drop(indexes_to_drop)\n",
    "            logger.info(f\"{filename}: Removed {len(indexes_to_drop)} duplicate rows with outputGroup 'N/A'.\")\n",
    "\n",
    "        # **新增功能结束**\n",
    "\n",
    "        # 生成输出文件路径，使用 os.path.splitext 确保正确替换扩展名\n",
    "        output_filename = f\"{os.path.splitext(filename)[0]}.csv\"\n",
    "        output_path = os.path.join(batch_folder, output_filename)\n",
    "\n",
    "        # 保存 'Flow Data' 相关的记录到 CSV 文件\n",
    "        try:\n",
    "            # 即使 flow_data 为空，也生成一个包含表头的 CSV 文件\n",
    "            flow_data.to_csv(output_path, index=False, encoding='utf-8')\n",
    "            converted_files += 1\n",
    "            logger.info(f\"{filename}: Flow data processed and saved to '{output_path}'\")\n",
    "        except Exception as e:\n",
    "            failed_files += 1\n",
    "            failed_file_names.append(filename)\n",
    "            logger.error(f\"{filename}: Error saving CSV file - {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{filename}: Error processing file - {e}\")\n",
    "        failed_files += 1\n",
    "        failed_file_names.append(filename)\n",
    "\n",
    "def main():\n",
    "    global total_files, converted_files, failed_files, failed_file_names\n",
    "\n",
    "    # 第一遍遍历所有文件，构建全局映射\n",
    "    logger.info(\"Starting first pass to build global activity mapping.\")\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".spold\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            logger.info(f\"Building mapping from file {filename}\")\n",
    "            file_mapping = extract_activity_mapping(file_path)\n",
    "            global_activity_mapping.update(file_mapping)\n",
    "\n",
    "    logger.info(f\"Global activity mapping built with {len(global_activity_mapping)} entries.\")\n",
    "\n",
    "    # 输出全局映射字典到CSV文件以便检查\n",
    "    mapping_output_path = os.path.join(batch_folder, \"global_activity_mapping.csv\")\n",
    "    try:\n",
    "        with open(mapping_output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['activity_id', 'shortname', 'activityName'])\n",
    "            for activity_id, (shortname, activityName) in global_activity_mapping.items():\n",
    "                writer.writerow([activity_id, shortname, activityName])\n",
    "        logger.info(f\"Global activity mapping exported to {mapping_output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exporting global activity mapping to CSV: {e}\")\n",
    "\n",
    "    # 第二遍遍历所有文件，处理数据\n",
    "    logger.info(\"Starting second pass to process all files.\")\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".spold\"):\n",
    "            total_files += 1\n",
    "            logger.info(f\"Processing file {filename}\")\n",
    "\n",
    "            try:\n",
    "                process_file(filename)\n",
    "            except Exception as e:\n",
    "                failed_files += 1\n",
    "                failed_file_names.append(filename)\n",
    "                logger.error(f\"{filename}: Error processing file - {e}\")\n",
    "                continue\n",
    "\n",
    "    # 生成失败文件列表并保存到文件\n",
    "    failed_files_log_path = os.path.join(batch_folder, \"failed_files.txt\")\n",
    "    try:\n",
    "        with open(failed_files_log_path, 'w', encoding='utf-8') as f:\n",
    "            for fname in failed_file_names:\n",
    "                f.write(f\"{fname}\\n\")\n",
    "        logger.info(f\"Failed file names saved to {failed_files_log_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing failed files log: {e}\")\n",
    "\n",
    "    # 生成处理总结并保存到 summary.txt，包括删除的行数统计和 process_name 替换统计\n",
    "    summary_log_path = os.path.join(batch_folder, \"summary.txt\")\n",
    "    try:\n",
    "        with open(summary_log_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=== Processing Summary ===\\n\")\n",
    "            f.write(f\"Total .spold files found: {total_files}\\n\")\n",
    "            f.write(f\"Successfully converted to CSV: {converted_files}\\n\")\n",
    "            f.write(f\"Failed to convert: {failed_files}\\n\\n\")\n",
    "            if failed_files > 0:\n",
    "                f.write(\"List of failed files:\\n\")\n",
    "                for fname in failed_file_names:\n",
    "                    f.write(f\"- {fname}\\n\")\n",
    "            f.write(\"\\n=== Deleted Rows Per File ===\\n\")\n",
    "            for fname, count in deleted_rows_per_file.items():\n",
    "                f.write(f\"{fname}: {count} rows deleted\\n\")\n",
    "            f.write(\"\\n=== Replaced Process Name Per File ===\\n\")\n",
    "            for fname, flow in replaced_process_name.items():\n",
    "                f.write(f\"{fname}: process_name replaced with flow '{flow}'\\n\")\n",
    "            f.write(\"\\n=== Non-1 Amount Without activityLinkId Per File ===\\n\")\n",
    "            for fname, count in non1_amount_per_file.items():\n",
    "                f.write(f\"{fname}: {count} rows with amount != 1 and no activityLinkId\\n\")\n",
    "            f.write(\"\\n=== Amount=-1 Without activityLinkId Per File ===\\n\")\n",
    "            for fname, count in neg1_amount_per_file.items():\n",
    "                f.write(f\"{fname}: {count} rows with amount=-1 and no activityLinkId\\n\")\n",
    "            f.write(\"\\n=== Files with Unknown Activity Name After Deletion ===\\n\")\n",
    "            for fname in unknown_activity_files:\n",
    "                f.write(f\"{fname}\\n\")\n",
    "        logger.info(f\"Processing summary saved to {summary_log_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing summary log: {e}\")\n",
    "\n",
    "    # 保存 non1_amount_per_file 到 CSV\n",
    "    output_non1_path = os.path.join(batch_folder, \"non1_amount_files.csv\")\n",
    "    try:\n",
    "        with open(output_non1_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['Filename', 'Non-1 Amount Count'])\n",
    "            for fname, count in non1_amount_per_file.items():\n",
    "                if count > 0:\n",
    "                    writer.writerow([fname, count])\n",
    "        logger.info(f\"Non-1 amount files saved to {output_non1_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exporting non1 amount files to CSV: {e}\")\n",
    "\n",
    "    # 保存 neg1_amount_per_file 到 CSV\n",
    "    output_neg1_path = os.path.join(batch_folder, \"neg1_amount_files.csv\")\n",
    "    try:\n",
    "        with open(output_neg1_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['Filename', 'Amount=-1 Count'])\n",
    "            for fname, count in neg1_amount_per_file.items():\n",
    "                if count > 0:\n",
    "                    writer.writerow([fname, count])\n",
    "        logger.info(f\"Amount=-1 files saved to {output_neg1_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exporting amount=-1 files to CSV: {e}\")\n",
    "\n",
    "    # 统计结果并记录到 summary.txt\n",
    "    logger.info(\"=== Processing Summary ===\")\n",
    "    logger.info(f\"Total .spold files found: {total_files}\")\n",
    "    logger.info(f\"Successfully converted to CSV: {converted_files}\")\n",
    "    logger.info(f\"Failed to convert: {failed_files}\")\n",
    "\n",
    "    if failed_files > 0:\n",
    "        logger.info(\"List of failed files:\")\n",
    "        for fname in failed_file_names:\n",
    "            logger.info(f\"- {fname}\")\n",
    "\n",
    "    # 记录每个文件被删除的行数\n",
    "    logger.info(\"=== Deleted Rows Per File ===\")\n",
    "    for fname, count in deleted_rows_per_file.items():\n",
    "        logger.info(f\"{fname}: {count} rows deleted\")\n",
    "\n",
    "    # 记录每个文件被替换的 process_name\n",
    "    logger.info(\"=== Replaced Process Name Per File ===\")\n",
    "    for fname, flow in replaced_process_name.items():\n",
    "        logger.info(f\"{fname}: process_name replaced with flow '{flow}'\")\n",
    "\n",
    "    # 记录没有 activityLinkId 且 amount !=1 的行数\n",
    "    logger.info(\"=== Non-1 Amount Without activityLinkId Per File ===\")\n",
    "    for fname, count in non1_amount_per_file.items():\n",
    "        logger.info(f\"{fname}: {count} rows with amount != 1 and no activityLinkId\")\n",
    "\n",
    "    # 记录没有 activityLinkId 且 amount ==-1 的行数\n",
    "    logger.info(\"=== Amount=-1 Without activityLinkId Per File ===\")\n",
    "    for fname, count in neg1_amount_per_file.items():\n",
    "        logger.info(f\"{fname}: {count} rows with amount=-1 and no activityLinkId\")\n",
    "\n",
    "    # 记录仍有 [Unknown Location]Unknown Activity Name 的文件\n",
    "    logger.info(\"=== Files with Unknown Activity Name After Deletion ===\")\n",
    "    for fname in unknown_activity_files:\n",
    "        logger.info(f\"{fname}\")\n",
    "\n",
    "    # 增加批次编号以便下一次运行\n",
    "    try:\n",
    "        with open(batch_file_path, \"w\") as f:\n",
    "            f.write(str(batch_number + 1))\n",
    "        logger.info(f\"Batch number incremented to {batch_number + 1}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating batch number: {e}\")\n",
    "\n",
    "    logger.info(\"All files processed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "346817ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files are missing in dir1 compared to dir2.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def compare_directories(dir1, dir2):\n",
    "    \"\"\"\n",
    "    Compare two directories and find files present in dir2 but missing in dir1.\n",
    "    \n",
    "    Args:\n",
    "        dir1 (str): The path to the first directory.\n",
    "        dir2 (str): The path to the second directory.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of files present in dir2 but missing in dir1.\n",
    "    \"\"\"\n",
    "    # List files in both directories\n",
    "    files_dir1 = set(os.listdir(dir1))\n",
    "    files_dir2 = set(os.listdir(dir2))\n",
    "    \n",
    "    # Find files present in dir2 but missing in dir1\n",
    "    missing_files = files_dir2 - files_dir1\n",
    "    return list(missing_files)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the directories to compare\n",
    "    dir1 = r\"C:\\Users\\WasteWang\\LCA\\OUTPUT\\1213_93\"\n",
    "    dir2 = r\"C:\\Users\\WasteWang\\LCA\\OUTPUT\\1211_88\"\n",
    "    \n",
    "    # Compare directories\n",
    "    missing_files = compare_directories(dir1, dir2)\n",
    "    \n",
    "    # Print results\n",
    "    if missing_files:\n",
    "        print(\"Files present in dir2 but missing in dir1:\")\n",
    "        for file in missing_files:\n",
    "            print(file)\n",
    "    else:\n",
    "        print(\"No files are missing in dir1 compared to dir2.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e3861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总共有 26037 个CSV文件需要处理。\n",
      "已处理 1000/26037 个文件。\n",
      "已处理 2000/26037 个文件。\n",
      "已处理 3000/26037 个文件。\n",
      "已处理 4000/26037 个文件。\n",
      "已处理 5000/26037 个文件。\n",
      "已处理 6000/26037 个文件。\n",
      "已处理 7000/26037 个文件。\n",
      "已处理 8000/26037 个文件。\n",
      "已处理 9000/26037 个文件。\n",
      "已处理 10000/26037 个文件。\n",
      "已处理 11000/26037 个文件。\n",
      "已处理 12000/26037 个文件。\n",
      "已处理 13000/26037 个文件。\n",
      "已处理 14000/26037 个文件。\n",
      "已处理 15000/26037 个文件。\n",
      "已处理 16000/26037 个文件。\n",
      "已处理 17000/26037 个文件。\n",
      "已处理 18000/26037 个文件。\n",
      "已处理 19000/26037 个文件。\n",
      "已处理 20000/26037 个文件。\n",
      "已处理 21000/26037 个文件。\n",
      "已处理 22000/26037 个文件。\n",
      "已处理 23000/26037 个文件。\n",
      "已处理 24000/26037 个文件。\n",
      "已处理 25000/26037 个文件。\n",
      "已处理 26000/26037 个文件。\n",
      "警告: 文件 global_activity_mapping.csv 缺少必要的列。\n",
      "警告: 文件 neg1_amount_files.csv 缺少必要的列。\n",
      "警告: 文件 non1_amount_files.csv 缺少必要的列。\n",
      "构建DataFrame中...\n",
      "保存矩阵到 C:\\Users\\WasteWang\\LCA\\OUTPUT\\NEW_3.11_1213_93_LCA_matrix.csv 中...\n",
      "完成矩阵构建！\n",
      "\n",
      "===== 矩阵统计信息 =====\n",
      "行数: 29271\n",
      "列数: 26034\n",
      "非零值数量: 1645996\n",
      "非零值占比: 0.22%\n",
      "负值数量: 183324\n",
      "负值占比: 0.02%\n",
      "负值占非零值比重: 11.14%\n",
      "========================\n",
      "\n",
      "全零行的数量: 0\n",
      "\n",
      "以下CSV文件未生成新列，详情请查看 'C:\\Users\\WasteWang\\LCA\\OUTPUT\\failed_files.txt' 文件：\n",
      " - global_activity_mapping.csv\n",
      " - neg1_amount_files.csv\n",
      " - non1_amount_files.csv\n",
      "没有发现重复的 process_name。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "def build_lca_matrix(folder_path, output_file):\n",
    "    # 用于按处理顺序记录process_name的列表\n",
    "    process_name_list = []\n",
    "    # 初始化有序字典以保持插入顺序\n",
    "    data_dict = OrderedDict()\n",
    "    flow_set = set()\n",
    "\n",
    "    # 列出所有CSV文件路径\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    total_files = len(csv_files)\n",
    "    print(f\"总共有 {total_files} 个CSV文件需要处理。\")\n",
    "\n",
    "    # 用于记录未成功处理的文件\n",
    "    failed_files = []\n",
    "    # 用于检测重复的process_name\n",
    "    process_name_set = set()\n",
    "    duplicate_process_files = []\n",
    "\n",
    "    for idx, file_name in enumerate(csv_files, 1):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            # 尝试用UTF-8读取\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            # 如果UTF-8失败，尝试gbk\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding='gbk')\n",
    "            except Exception as e:\n",
    "                print(f\"错误: 无法读取文件 {file_name}。原因: {e}\")\n",
    "                failed_files.append(file_name)\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"错误: 无法读取文件 {file_name}。原因: {e}\")\n",
    "            failed_files.append(file_name)\n",
    "            continue\n",
    "\n",
    "        # 检查必需列\n",
    "        if not {'process_name', 'field', 'flow', 'amount'}.issubset(df.columns):\n",
    "            print(f\"警告: 文件 {file_name} 缺少必要的列。\")\n",
    "            failed_files.append(file_name)\n",
    "            continue\n",
    "\n",
    "        process_names = df['process_name'].unique()\n",
    "        if len(process_names) < 1:\n",
    "            print(f\"警告: 文件 {file_name} 无法找到process_name。\")\n",
    "            failed_files.append(file_name)\n",
    "            continue\n",
    "\n",
    "        process_name = process_names[0]\n",
    "        if len(process_names) > 1:\n",
    "            print(f\"警告: 文件 {file_name} 中存在多个process_name，将使用第一个: {process_name}\")\n",
    "\n",
    "        # 检查是否有重复的process_name\n",
    "        if process_name in process_name_set:\n",
    "            print(f\"警告: 重复的process_name '{process_name}' 出现在文件 {file_name} 中，跳过该文件。\")\n",
    "            duplicate_process_files.append(file_name)\n",
    "            failed_files.append(file_name)\n",
    "            continue\n",
    "        else:\n",
    "            process_name_set.add(process_name)\n",
    "            # 将process_name按照处理顺序加入列表\n",
    "            process_name_list.append(process_name)\n",
    "\n",
    "        # 过滤需要的字段\n",
    "        df_filtered = df[df['field'].isin(['intermediateExchange', 'elementaryExchange'])]\n",
    "\n",
    "        if df_filtered.empty:\n",
    "            print(f\"警告: 文件 {file_name} 过滤后没有有效的行。\")\n",
    "            failed_files.append(file_name)\n",
    "            continue\n",
    "\n",
    "        # 分离 intermediate 和 elementary\n",
    "        df_intermediate = df_filtered[df_filtered['field'] == 'intermediateExchange']\n",
    "        df_elementary = df_filtered[df_filtered['field'] == 'elementaryExchange']\n",
    "\n",
    "        if df_intermediate.empty and df_elementary.empty:\n",
    "            print(f\"警告: 文件 {file_name} 中没有 intermediateExchange 或 elementaryExchange 的行。\")\n",
    "            failed_files.append(file_name)\n",
    "            continue\n",
    "\n",
    "        column_added = False\n",
    "\n",
    "        # 处理 intermediateExchange\n",
    "        for _, row in df_intermediate.iterrows():\n",
    "            flow = row['flow']\n",
    "            amount = row['amount'] if pd.notnull(row['amount']) else 0\n",
    "            if flow not in flow_set:\n",
    "                flow_set.add(flow)\n",
    "                data_dict[flow] = {}\n",
    "            data_dict[flow][process_name] = amount\n",
    "            column_added = True\n",
    "\n",
    "        # 处理 elementaryExchange\n",
    "        for _, row in df_elementary.iterrows():\n",
    "            flow = row['flow']\n",
    "            amount = row['amount'] if pd.notnull(row['amount']) else 0\n",
    "            if flow not in flow_set:\n",
    "                flow_set.add(flow)\n",
    "                data_dict[flow] = {}\n",
    "            data_dict[flow][process_name] = amount\n",
    "            column_added = True\n",
    "\n",
    "        if not column_added:\n",
    "            failed_files.append(file_name)\n",
    "\n",
    "        if idx % 1000 == 0 or idx == total_files:\n",
    "            print(f\"已处理 {idx}/{total_files} 个文件。\")\n",
    "\n",
    "    # 合并数据构建矩阵\n",
    "    print(\"构建DataFrame中...\")\n",
    "    all_flows = list(data_dict.keys())\n",
    "    matrix_df = pd.DataFrame.from_dict(data_dict, orient='index').fillna(0)\n",
    "    # 按插入顺序排序行\n",
    "    matrix_df = matrix_df.reindex(all_flows)\n",
    "    # 使用 process_name_list 来保证列顺序\n",
    "    matrix_df = matrix_df.reindex(columns=process_name_list)\n",
    "    matrix_df.index.name = 'flow'\n",
    "\n",
    "    # 对角化处理：\n",
    "    # 如果某列的列名在行名中存在，则将对应的行移动至与列同索引（对角线）位置\n",
    "    row_order = list(matrix_df.index)\n",
    "    for i, col in enumerate(matrix_df.columns):\n",
    "        if col in row_order:\n",
    "            current_pos = row_order.index(col)\n",
    "            if current_pos != i:\n",
    "                # 交换行，使得 (col, col) 处于对角线上\n",
    "                row_order[i], row_order[current_pos] = row_order[current_pos], row_order[i]\n",
    "    matrix_df = matrix_df.reindex(index=row_order)\n",
    "\n",
    "    # 保存到CSV\n",
    "    print(f\"保存矩阵到 {output_file} 中...\")\n",
    "    matrix_df.to_csv(output_file, encoding='utf-8')\n",
    "    print(\"完成矩阵构建！\")\n",
    "\n",
    "    # 统计信息\n",
    "    print(\"\\n===== 矩阵统计信息 =====\")\n",
    "    num_rows, num_cols = matrix_df.shape\n",
    "    total_elements = num_rows * num_cols\n",
    "    non_zero = (matrix_df != 0).sum().sum()\n",
    "    non_zero_ratio = non_zero / total_elements if total_elements != 0 else 0\n",
    "    negative = (matrix_df < 0).sum().sum()\n",
    "    negative_ratio = negative / total_elements if total_elements != 0 else 0\n",
    "    negative_over_non_zero = (negative / non_zero) if non_zero != 0 else 0\n",
    "\n",
    "    print(f\"行数: {num_rows}\")\n",
    "    print(f\"列数: {num_cols}\")\n",
    "    print(f\"非零值数量: {non_zero}\")\n",
    "    print(f\"非零值占比: {non_zero_ratio:.2%}\")\n",
    "    print(f\"负值数量: {negative}\")\n",
    "    print(f\"负值占比: {negative_ratio:.2%}\")\n",
    "    print(f\"负值占非零值比重: {negative_over_non_zero:.2%}\")\n",
    "    print(\"========================\\n\")\n",
    "\n",
    "    # 统计全零行数量\n",
    "    all_zero_rows = (matrix_df.sum(axis=1) == 0).sum()\n",
    "    print(f\"全零行的数量: {all_zero_rows}\")\n",
    "\n",
    "    # 输出未生成新列的文件列表\n",
    "    if failed_files:\n",
    "        failed_file_path = os.path.join(os.path.dirname(output_file), \"failed_files.txt\")\n",
    "        with open(failed_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"未生成新列的CSV文件列表:\\n\")\n",
    "            for file in failed_files:\n",
    "                f.write(f\"{file}\\n\")\n",
    "        print(f\"\\n以下CSV文件未生成新列，详情请查看 '{failed_file_path}' 文件：\")\n",
    "        for file in failed_files:\n",
    "            print(f\" - {file}\")\n",
    "    else:\n",
    "        print(\"所有CSV文件均已成功生成新列。\")\n",
    "\n",
    "    # 输出重复process_name的CSV文件\n",
    "    if duplicate_process_files:\n",
    "        duplicate_file_path = os.path.join(os.path.dirname(output_file), \"duplicate_process_files.txt\")\n",
    "        with open(duplicate_file_path, \"w\", encoding='utf-8') as f:\n",
    "            f.write(\"具有重复 process_name 的CSV文件列表:\\n\")\n",
    "            for file in duplicate_process_files:\n",
    "                f.write(f\"{file}\\n\")\n",
    "        print(f\"\\n以下CSV文件具有重复的 process_name，未生成新列，详情请查看 '{duplicate_file_path}' 文件：\")\n",
    "        for file in duplicate_process_files:\n",
    "            print(f\" - {file}\")\n",
    "    else:\n",
    "        print(\"没有发现重复的 process_name。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = r\"C:\\Users\\WasteWang\\LCA\\OUTPUT\\1213_93\"\n",
    "    output_file = r\"C:\\Users\\WasteWang\\LCA\\OUTPUT\\NEW_3.11_1213_93_LCA_matrix.csv\"\n",
    "    # 每运行一次这里要改一次文件名\n",
    "    build_lca_matrix(folder_path, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
