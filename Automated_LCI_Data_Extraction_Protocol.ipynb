{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyecospold import parse_file_v2\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "# Global mapping dictionary from activity_id to (shortname, activityName)\n",
    "global_activity_mapping = {}\n",
    "\n",
    "# Initialize statistical variables\n",
    "total_files = 0\n",
    "converted_files = 0\n",
    "failed_files = 0\n",
    "failed_file_names = []\n",
    "\n",
    "# Used to track the number of deleted rows in each file\n",
    "deleted_rows_per_file = {}\n",
    "\n",
    "# Used to track the replaced process_name in each file\n",
    "replaced_process_name = {}\n",
    "\n",
    "# Used to count the number of rows where activityLinkId is missing and amount is not 1\n",
    "non1_amount_per_file = {}\n",
    "\n",
    "# Used to count the number of rows where activityLinkId is missing and amount is -1\n",
    "neg1_amount_per_file = {}\n",
    "\n",
    "# Used to record files that still have [Unknown Location]Unknown Activity Name\n",
    "unknown_activity_files = []\n",
    "\n",
    "# Define input and output folder paths\n",
    "input_folder = \"C:\\\\Users\\\\WasteWang\\\\LCA\\\\DATA\\\\3.11_APOS\\\\datasets\"\n",
    "output_folder_base = \"C:\\\\Users\\\\WasteWang\\\\LCA\\\\OUTPUT\"\n",
    "lookup_file_path = \"C:\\\\Users\\\\WasteWang\\\\LCA\\\\DATA\\\\3.11_APOS\\\\FilenameToActivityLookup.csv\"\n",
    "batch_file_path = \"C:\\\\Users\\\\WasteWang\\\\LCA\\\\batch_number.txt\"  # File path for storing the batch number\n",
    "\n",
    "# If the batch number file does not exist, initialize it to 1\n",
    "if not os.path.exists(batch_file_path):\n",
    "    try:\n",
    "        with open(batch_file_path, \"w\") as f:\n",
    "            f.write(\"1\")\n",
    "        print(\"Batch number file created with initial value 1.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating batch number file: {e}\")\n",
    "\n",
    "# Read the batch number\n",
    "try:\n",
    "    with open(batch_file_path, \"r\") as f:\n",
    "        batch_number = int(f.read().strip())\n",
    "    print(f\"Current batch number: {batch_number}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading batch number file: {e}\")\n",
    "    batch_number = 1  # Default value\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.now().strftime(\"%m%d\")\n",
    "print(f\"Current date: {current_date}\")\n",
    "\n",
    "# Create the output folder for the current batch\n",
    "batch_folder = os.path.join(output_folder_base, f\"{current_date}_{batch_number}\")\n",
    "os.makedirs(batch_folder, exist_ok=True)\n",
    "print(f\"Output will be saved to: {batch_folder}\")\n",
    "\n",
    "# Configure logging, output to file only, set to INFO level\n",
    "logger = logging.getLogger('spold_processor')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a file handler to save to processing_debug.txt in the output batch folder\n",
    "file_handler = logging.FileHandler(os.path.join(batch_folder, 'processing_debug.txt'), encoding='utf-8')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s:%(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Load the mapping table from filename to activity name, specify semicolon delimiter\n",
    "try:\n",
    "    lookup_df = pd.read_csv(lookup_file_path, sep=';')\n",
    "    lookup_dict = {row['Filename'].split('_')[0]: (row['ActivityName'], row['Location']) for _, row in lookup_df.iterrows()}\n",
    "    logger.info(f\"Loaded lookup dictionary with {len(lookup_dict)} entries.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading lookup file: {e}\")\n",
    "    lookup_dict = {}\n",
    "\n",
    "# Verify the integrity of lookup_dict\n",
    "def verify_lookup_dict():\n",
    "    missing_prefixes = set()\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".spold\"):\n",
    "            prefix = filename.split('_')[0] if '_' in filename else os.path.splitext(filename)[0]\n",
    "            if prefix not in lookup_dict:\n",
    "                missing_prefixes.add(prefix)\n",
    "    if missing_prefixes:\n",
    "        logger.warning(f\"The following prefixes are missing in lookup_dict: {', '.join(missing_prefixes)}\")\n",
    "    else:\n",
    "        logger.info(\"All prefixes are present in lookup_dict.\")\n",
    "\n",
    "verify_lookup_dict()\n",
    "\n",
    "def extract_activity_mapping(file_path):\n",
    "    \"\"\"\n",
    "    Extracts all activityDescription elements from a single .spold file and returns a mapping dictionary.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    try:\n",
    "        logger.info(f\"Parsing file: {file_path}\")\n",
    "        ecoSpold = parse_file_v2(file_path)\n",
    "        namespaces = {'eco': 'http://www.EcoInvent.org/EcoSpold02'}\n",
    "        \n",
    "        # Get all activityDescription elements, including sub-activities\n",
    "        activity_descriptions = ecoSpold.findall('.//eco:activityDescription', namespaces)\n",
    "        for activity_description in activity_descriptions:\n",
    "            activity = activity_description.find('eco:activity', namespaces)\n",
    "            if activity is not None:\n",
    "                activity_id = activity.attrib.get('id')\n",
    "                activity_name_elem = activity.find('eco:activityName', namespaces)\n",
    "                activity_name_text = activity_name_elem.text.strip() if activity_name_elem is not None and activity_name_elem.text else \"Unknown Activity Name\"\n",
    "\n",
    "                geography = activity_description.find('eco:geography', namespaces)\n",
    "                if geography is not None:\n",
    "                    shortname_elem = geography.find('eco:shortname', namespaces)\n",
    "                    shortname = shortname_elem.text.strip() if shortname_elem is not None and shortname_elem.text else \"Unknown Location\"\n",
    "                else:\n",
    "                    shortname = \"Unknown Location\"\n",
    "\n",
    "                if activity_id:\n",
    "                    mapping[activity_id] = (shortname, activity_name_text)\n",
    "                    logger.info(f\"Mapped activity_id {activity_id} to ({shortname}, {activity_name_text})\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting activity mapping from file {file_path}: {e}\")\n",
    "    return mapping\n",
    "\n",
    "def process_activity_description(activityDescription, current_activity_name, namespaces):\n",
    "    \"\"\"\n",
    "    Processes the information in the activityDescription section and returns a list of dictionary records.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    try:\n",
    "        if activityDescription is not None:\n",
    "            for field, tag in [('includedActivitiesStart', 'includedActivitiesStart'),\n",
    "                               ('includedActivitiesEnd', 'includedActivitiesEnd'),\n",
    "                               ('generalComment', 'generalComment')]:\n",
    "                value = activityDescription.find(f'eco:{tag}', namespaces)\n",
    "                value_text = value.text.strip() if value is not None and value.text else 'N/A'\n",
    "                category = 'Included Activities' if field != 'generalComment' else 'General Comment'\n",
    "                record = {\n",
    "                    'process_name': current_activity_name,\n",
    "                    'flow': '',  # This field has no data in this section\n",
    "                    'unit': '',\n",
    "                    'amount': '',\n",
    "                    'category': category,\n",
    "                    'field': field,\n",
    "                    'value': value_text,\n",
    "                    'compartment': '',\n",
    "                    'subcompartment': '',\n",
    "                    'comment': '',\n",
    "                    'outputGroup': '',\n",
    "                    'section': 'activityDescription',\n",
    "                    'activityLinkId': '',\n",
    "                    'intermediateExchangeId': ''\n",
    "                }\n",
    "                records.append(record)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing activityDescription: {e}\")\n",
    "    return records\n",
    "\n",
    "def process_intermediate_exchange(exchange, current_activity_name, current_location, filename, file_activity_mapping, namespaces):\n",
    "    \"\"\"\n",
    "    Processes an intermediateExchange element and returns a dictionary record.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        intermediateExchangeId = exchange.attrib.get('intermediateExchangeId', '').strip()\n",
    "        amount_str = exchange.attrib.get('amount', 'N/A').strip()\n",
    "        try:\n",
    "            amount = float(amount_str)\n",
    "        except ValueError:\n",
    "            amount = None\n",
    "            logger.warning(f\"{filename}: Invalid amount value '{amount_str}' in intermediateExchangeId '{intermediateExchangeId}'.\")\n",
    "        \n",
    "        flow_name_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}name')\n",
    "        flow_name = flow_name_elem.text.strip() if flow_name_elem is not None and flow_name_elem.text else \"Unknown Flow\"\n",
    "        unit_name_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}unitName')\n",
    "        unit_name = unit_name_elem.text.strip() if unit_name_elem is not None and unit_name_elem.text else \"Unknown Unit\"\n",
    "        comment_field = exchange.find('{http://www.EcoInvent.org/EcoSpold02}comment')\n",
    "        comment = comment_field.text.strip() if comment_field is not None and comment_field.text else 'N/A'\n",
    "        outputGroup_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}outputGroup')\n",
    "        outputGroup = outputGroup_elem.text.strip() if outputGroup_elem is not None and outputGroup_elem.text else 'N/A'\n",
    "        activityLinkId = exchange.attrib.get('activityLinkId', '').strip()\n",
    "\n",
    "        # Condition: For intermediateExchange, if amount == 0, delete the row\n",
    "        if amount == 0.0:\n",
    "            logger.info(f\"{filename}: Skipping intermediateExchange with amount=0.0.\")\n",
    "            return None  # Do not add this record\n",
    "\n",
    "        # Use activityLinkId to get the corresponding shortname and activityName\n",
    "        if activityLinkId:\n",
    "            related_info = global_activity_mapping.get(activityLinkId)\n",
    "            if not related_info:\n",
    "                # Try to get from the current file's mapping\n",
    "                related_info = file_activity_mapping.get(activityLinkId)\n",
    "            if not related_info:\n",
    "                related_info = (\"Unknown Location\", \"Unknown Activity Name\")\n",
    "                logger.warning(f\"{filename}: activityLinkId '{activityLinkId}' not found in global or file mapping.\")\n",
    "        else:\n",
    "            # If there is no activityLinkId, use the current file's shortname and activityName\n",
    "            related_info = (current_location, current_activity_name)\n",
    "\n",
    "        related_shortname, related_activity_name = related_info\n",
    "\n",
    "        # Format the flow field\n",
    "        formatted_flow = f\"{flow_name}//[{related_shortname}]{related_activity_name}\"\n",
    "\n",
    "        # Create a record dictionary\n",
    "        record = {\n",
    "            'process_name': current_activity_name,\n",
    "            'flow': formatted_flow,\n",
    "            'unit': unit_name,\n",
    "            'amount': amount,\n",
    "            'category': 'Flow Data',\n",
    "            'field': 'intermediateExchange',\n",
    "            'value': '',\n",
    "            'compartment': '',\n",
    "            'subcompartment': '',\n",
    "            'comment': comment,\n",
    "            'outputGroup': outputGroup,\n",
    "            'section': 'intermediateExchange',\n",
    "            'activityLinkId': activityLinkId,\n",
    "            'intermediateExchangeId': intermediateExchangeId\n",
    "        }\n",
    "        return record\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{filename}: Error processing intermediateExchange - {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_elementary_exchange(exchange, current_activity_name, filename, file_activity_mapping, namespaces):\n",
    "    \"\"\"\n",
    "    Processes an elementaryExchange element and returns a dictionary record.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        flow_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}name')\n",
    "        flow = flow_elem.text.strip() if flow_elem is not None and flow_elem.text else \"Unknown Flow\"\n",
    "        unit_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}unitName')\n",
    "        unit = unit_elem.text.strip() if unit_elem is not None and unit_elem.text else \"Unknown Unit\"\n",
    "        amount_str = exchange.attrib.get('amount', 'N/A').strip()\n",
    "        try:\n",
    "            amount = float(amount_str)\n",
    "        except ValueError:\n",
    "            amount = None\n",
    "            logger.warning(f\"{filename}: Invalid amount value '{amount_str}' in elementaryExchangeId '{exchange.attrib.get('intermediateExchangeId', '').strip()}'.\")\n",
    "    \n",
    "        comment_field = exchange.find('{http://www.EcoInvent.org/EcoSpold02}comment')\n",
    "        comment = comment_field.text.strip() if comment_field is not None and comment_field.text else 'N/A'\n",
    "\n",
    "        # Condition: For elementaryExchange, if amount == 0, delete the row\n",
    "        if amount == 0.0:\n",
    "            logger.info(f\"{filename}: Skipping elementaryExchange with amount=0.0.\")\n",
    "            return None  # Do not add this record\n",
    "\n",
    "        # Process compartment and subcompartment\n",
    "        compartment = exchange.find('{http://www.EcoInvent.org/EcoSpold02}compartment')\n",
    "        if compartment is not None:\n",
    "            compartment_main = compartment.find('{http://www.EcoInvent.org/EcoSpold02}compartment')\n",
    "            compartment_text = compartment_main.text.strip() if compartment_main is not None and compartment_main.text else 'Unknown Compartment'\n",
    "            subcompartment_elem = compartment.find('{http://www.EcoInvent.org/EcoSpold02}subcompartment')\n",
    "            subcompartment = subcompartment_elem.text.strip() if subcompartment_elem is not None and subcompartment_elem.text else 'Unknown Subcompartment'\n",
    "        else:\n",
    "            compartment_text = 'Unknown Compartment'\n",
    "            subcompartment = 'Unknown Subcompartment'\n",
    "\n",
    "        outputGroup_elem = exchange.find('{http://www.EcoInvent.org/EcoSpold02}outputGroup')\n",
    "        outputGroup = outputGroup_elem.text.strip() if outputGroup_elem is not None and outputGroup_elem.text else 'N/A'\n",
    "        activityLinkId = exchange.attrib.get('activityLinkId', '').strip()\n",
    "        intermediateExchangeId = exchange.attrib.get('intermediateExchangeId', '').strip()\n",
    "\n",
    "        # Use activityLinkId to get the corresponding shortname and activityName\n",
    "        if activityLinkId:\n",
    "            related_info = global_activity_mapping.get(activityLinkId)\n",
    "            if not related_info:\n",
    "                # Try to get from the current file's mapping\n",
    "                related_info = file_activity_mapping.get(activityLinkId)\n",
    "            if not related_info:\n",
    "                related_info = (\"Unknown Location\", \"Unknown Activity Name\")\n",
    "                logger.warning(f\"{filename}: activityLinkId '{activityLinkId}' not found in global or file mapping.\")\n",
    "        else:\n",
    "            # If there is no activityLinkId, use the current file's shortname and activityName\n",
    "            related_info = (current_activity_name, current_activity_name)  # It is assumed here that the current activity information is used when there is no activityLinkId\n",
    "\n",
    "        related_shortname, related_activity_name = related_info\n",
    "\n",
    "        # Format the flow field\n",
    "        formatted_flow = f\"{flow}//[{related_shortname}]{related_activity_name}\"\n",
    "\n",
    "        # Add compartment and subcompartment in the flow\n",
    "        formatted_flow = f\"{formatted_flow}_{compartment_text}_{subcompartment}\"\n",
    "\n",
    "        # Create a record dictionary\n",
    "        record = {\n",
    "            'process_name': current_activity_name,\n",
    "            'flow': formatted_flow,\n",
    "            'unit': unit,\n",
    "            'amount': amount,\n",
    "            'category': 'Flow Data',\n",
    "            'field': 'elementaryExchange',\n",
    "            'value': '',\n",
    "            'compartment': compartment_text,\n",
    "            'subcompartment': subcompartment,\n",
    "            'comment': comment,\n",
    "            'outputGroup': outputGroup,\n",
    "            'section': 'elementaryExchange',\n",
    "            'activityLinkId': activityLinkId,\n",
    "            'intermediateExchangeId': intermediateExchangeId\n",
    "        }\n",
    "        return record\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{filename}: Error processing elementaryExchange - {e}\")\n",
    "        return {}\n",
    "\n",
    "def process_file(filename):\n",
    "    global converted_files, failed_files, failed_file_names\n",
    "    global deleted_rows_per_file, replaced_process_name, non1_amount_per_file, neg1_amount_per_file, global_activity_mapping, unknown_activity_files\n",
    "\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    logger.info(f\"Processing file {filename}\")\n",
    "\n",
    "    try:\n",
    "        # Get the file name prefix and suffix\n",
    "        if \"_\" in filename:\n",
    "            prefix, suffix_with_ext = filename.split(\"_\", 1)\n",
    "            suffix = os.path.splitext(suffix_with_ext)[0].strip()\n",
    "            logger.info(f\"{filename}: Filename split into prefix: '{prefix}' and suffix: '{suffix}'\")\n",
    "        else:\n",
    "            prefix = os.path.splitext(filename)[0].strip()\n",
    "            suffix = \"\"\n",
    "            logger.warning(f\"{filename}: Filename does not contain '_'. Using entire name as prefix.\")\n",
    "\n",
    "        # Get activityName and location\n",
    "        file_prefix = prefix  # The prefix has already been extracted\n",
    "        process_info = lookup_dict.get(file_prefix, (\"Unknown Activity\", \"Unknown Location\"))\n",
    "        current_activity_name, current_location = process_info\n",
    "        logger.info(f\"{filename}: Current activity: '{current_activity_name}', Location: '{current_location}'\")\n",
    "\n",
    "        # Initialize an empty list to store all records\n",
    "        records = []\n",
    "\n",
    "        # Initialize delete count\n",
    "        deleted_rows_per_file[filename] = 0\n",
    "\n",
    "        # Initialize non-1 amount count\n",
    "        non1_amount_per_file[filename] = 0\n",
    "\n",
    "        # Initialize -1 amount count\n",
    "        neg1_amount_per_file[filename] = 0\n",
    "\n",
    "        # Parse the .spold file\n",
    "        ecoSpold = parse_file_v2(file_path)\n",
    "        namespaces = {'eco': 'http://www.EcoInvent.org/EcoSpold02'}\n",
    "\n",
    "        # Extract the activity mapping of the current file\n",
    "        file_activity_mapping = extract_activity_mapping(file_path)\n",
    "\n",
    "        # Extract information from the activityDescription section\n",
    "        activityDescription = ecoSpold.find('.//eco:activityDescription', namespaces)\n",
    "        activity_records = process_activity_description(activityDescription, current_activity_name, namespaces)\n",
    "        records.extend(activity_records)\n",
    "        logger.info(f\"{filename}: Extracted {len(activity_records)} activityDescription records.\")\n",
    "\n",
    "        # Extract information from the intermediateExchange section\n",
    "        intermediate_exchanges = ecoSpold.findall('.//eco:intermediateExchange', namespaces)\n",
    "        logger.info(f\"{filename}: Found {len(intermediate_exchanges)} intermediateExchange elements.\")\n",
    "\n",
    "        # Identify all intermediateExchangeIds that match the suffix\n",
    "        if suffix:\n",
    "            matching_exchanges = [exchange for exchange in intermediate_exchanges if exchange.attrib.get('intermediateExchangeId', '').strip() == suffix]\n",
    "            logger.info(f\"{filename}: Found {len(matching_exchanges)} intermediateExchange elements matching suffix '{suffix}'.\")\n",
    "        else:\n",
    "            matching_exchanges = []\n",
    "            logger.warning(f\"{filename}: Suffix is empty. No intermediateExchangeId to match.\")\n",
    "\n",
    "        # Update activityLinkId according to logic\n",
    "        if len(matching_exchanges) == 1:\n",
    "            exchange_to_update = matching_exchanges[0]\n",
    "            exchange_to_update.attrib['activityLinkId'] = prefix\n",
    "            logger.info(f\"{filename}: Only one matching intermediateExchangeId. Updated activityLinkId to '{prefix}'.\")\n",
    "        elif len(matching_exchanges) > 1:\n",
    "            # Update all activityLinkIds with amount=1 or amount=-1\n",
    "            # First find all amount=1 or 1.0\n",
    "            amount_1_exchanges = [ex for ex in matching_exchanges if ex.attrib.get('amount', '').strip() in ['1', '1.0']]\n",
    "            if amount_1_exchanges:\n",
    "                for ex in amount_1_exchanges:\n",
    "                    ex.attrib['activityLinkId'] = prefix\n",
    "                    logger.info(f\"{filename}: Updated activityLinkId to '{prefix}' for intermediateExchangeId '{ex.attrib.get('intermediateExchangeId')}' with amount={ex.attrib.get('amount')}.\")\n",
    "            else:\n",
    "                # If there is no amount=1, then find amount=-1 or -1.0\n",
    "                amount_neg1_exchanges = [ex for ex in matching_exchanges if ex.attrib.get('amount', '').strip() in ['-1', '-1.0']]\n",
    "                if amount_neg1_exchanges:\n",
    "                    for ex in amount_neg1_exchanges:\n",
    "                        ex.attrib['activityLinkId'] = prefix\n",
    "                        logger.info(f\"{filename}: Updated activityLinkId to '{prefix}' for intermediateExchangeId '{ex.attrib.get('intermediateExchangeId')}' with amount={ex.attrib.get('amount')}.\")\n",
    "                else:\n",
    "                    logger.info(f\"{filename}: Multiple matches. No amount=1 or amount=-1. No changes made.\")\n",
    "\n",
    "        # Process all intermediateExchanges\n",
    "        for exchange in intermediate_exchanges:\n",
    "            record = process_intermediate_exchange(exchange, current_activity_name, current_location, filename, file_activity_mapping, namespaces)\n",
    "            if record:\n",
    "                records.append(record)\n",
    "            else:\n",
    "                deleted_rows_per_file[filename] += 1\n",
    "\n",
    "            # Count the cases where activityLinkId is missing and amount != 1\n",
    "            activityLinkId = exchange.attrib.get('activityLinkId', '').strip()\n",
    "            amount_str = exchange.attrib.get('amount', 'N/A').strip()\n",
    "            try:\n",
    "                amount = float(amount_str)\n",
    "            except ValueError:\n",
    "                amount = None\n",
    "                logger.warning(f\"{filename}: Invalid amount value '{amount_str}' in intermediateExchangeId '{exchange.attrib.get('intermediateExchangeId', '').strip()}'.\")\n",
    "        \n",
    "            if not activityLinkId and amount is not None and amount != 1.0:\n",
    "                non1_amount_per_file[filename] = non1_amount_per_file.get(filename, 0) + 1\n",
    "                logger.info(f\"{filename}: Found intermediateExchange with amount={amount} and no activityLinkId.\")\n",
    "                if amount == -1.0:\n",
    "                    neg1_amount_per_file[filename] = neg1_amount_per_file.get(filename, 0) + 1\n",
    "                    logger.info(f\"{filename}: Found intermediateExchange with amount=-1.0 and no activityLinkId.\")\n",
    "\n",
    "        # Extract information from the elementaryExchange section\n",
    "        elementary_exchanges = ecoSpold.findall('.//eco:elementaryExchange', namespaces)\n",
    "        logger.info(f\"{filename}: Found {len(elementary_exchanges)} elementaryExchange elements.\")\n",
    "        for exchange in elementary_exchanges:\n",
    "            record = process_elementary_exchange(exchange, current_activity_name, filename, file_activity_mapping, namespaces)\n",
    "            if record:  # Add only when processing is successful\n",
    "                records.append(record)\n",
    "            else:\n",
    "                deleted_rows_per_file[filename] += 1\n",
    "\n",
    "            # Count the cases where activityLinkId is missing and amount != 1\n",
    "            activityLinkId = exchange.attrib.get('activityLinkId', '').strip()\n",
    "            amount_str = exchange.attrib.get('amount', 'N/A').strip()\n",
    "            try:\n",
    "                amount = float(amount_str)\n",
    "            except ValueError:\n",
    "                amount = None\n",
    "                logger.warning(f\"{filename}: Invalid amount value '{amount_str}' in elementaryExchangeId '{exchange.attrib.get('intermediateExchangeId', '').strip()}'.\")\n",
    "    \n",
    "            if not activityLinkId and amount is not None and amount != 1.0:\n",
    "                non1_amount_per_file[filename] = non1_amount_per_file.get(filename, 0) + 1\n",
    "                logger.info(f\"{filename}: Found elementaryExchange with amount={amount} and no activityLinkId.\")\n",
    "                if amount == -1.0:\n",
    "                    neg1_amount_per_file[filename] = neg1_amount_per_file.get(filename, 0) + 1\n",
    "                    logger.info(f\"{filename}: Found elementaryExchange with amount=-1.0 and no activityLinkId.\")\n",
    "\n",
    "        # Create DataFrame\n",
    "        all_data = pd.DataFrame(records, columns=[\n",
    "            'process_name', 'flow', 'unit', 'amount', 'category', 'field', 'value',\n",
    "            'compartment', 'subcompartment', 'comment', 'outputGroup', 'section',\n",
    "            'activityLinkId', 'intermediateExchangeId'\n",
    "        ])\n",
    "\n",
    "        # Generate a combined string and check if it matches \"prefix_suffix\"\n",
    "        matched_flow = None\n",
    "        target_combined = f\"{prefix}_{suffix}\"\n",
    "        logger.info(f\"{filename}: Target combined string: '{target_combined}'\")\n",
    "\n",
    "        # Create a boolean condition\n",
    "        condition = (all_data['activityLinkId'].fillna('') + '_' + all_data['intermediateExchangeId'].fillna('')) == target_combined\n",
    "\n",
    "        if any(condition):\n",
    "            matched_flow = all_data.loc[condition, 'flow'].iloc[0]  # Assume there is only one match\n",
    "            logger.info(f\"{filename}: Matched combined string '{target_combined}'. Setting process_name to '{matched_flow}' for all records.\")\n",
    "\n",
    "        # Replace the process_name of the entire file based on the match result\n",
    "        if matched_flow:\n",
    "            all_data['process_name'] = matched_flow  # Replace the entire column\n",
    "            replaced_process_name[filename] = matched_flow\n",
    "            logger.info(f\"{filename}: process_name has been set to '{matched_flow}' for all records based on combination '{target_combined}'.\")\n",
    "\n",
    "        # Check if the flow field contains [Unknown Location]Unknown Activity Name\n",
    "        if any(all_data['flow'].str.contains(r'\\[Unknown Location\\]Unknown Activity Name', na=False)):\n",
    "            unknown_activity_files.append(filename)\n",
    "            logger.warning(f\"{filename}: Contains '[Unknown Location]Unknown Activity Name' in flow fields after processing.\")\n",
    "\n",
    "        # Select only records of the 'Flow Data' category and create a copy\n",
    "        flow_data = all_data[all_data['category'] == 'Flow Data'].copy()\n",
    "        logger.info(f\"{filename}: 'Flow Data' records count: {len(flow_data)}\")\n",
    "\n",
    "        # **New feature: Modify the content of the 'flow' column that meets the conditions**\n",
    "        # Define the function to modify the flow\n",
    "        def modify_flow(flow):\n",
    "            \"\"\"\n",
    "            Modify the content of the flow field:\n",
    "            If the flow contains \"//\", delete all content from \"//\" to the first \"_\"\n",
    "            and add a \"_\" between the remaining parts.\n",
    "            \"\"\"\n",
    "            if '//' in flow:\n",
    "                try:\n",
    "                    start = flow.index('//')\n",
    "                    # Find the first '_' after the start\n",
    "                    after_start = flow[start:].index('_')\n",
    "                    end = start + after_start + 1  # Include '_'\n",
    "                    # Delete the content from \"//\" to before the first \"_\" and add \"_\"\n",
    "                    modified_flow = flow[:start] + '_' + flow[end:]\n",
    "                    logger.debug(f\"Original flow: '{flow}' | Modified flow: '{modified_flow}'\")\n",
    "                    return modified_flow\n",
    "                except ValueError:\n",
    "                    # If '_' is not found, return the original flow\n",
    "                    logger.warning(f\"Flow '{flow}' contains '//' but no '_'. No modification applied.\")\n",
    "                    return flow\n",
    "            else:\n",
    "                return flow\n",
    "\n",
    "        # Apply the modify_flow function to the rows in the 'flow' column that meet the conditions\n",
    "        try:\n",
    "            # Apply modification only to rows with 'elementaryExchange'\n",
    "            condition_modify = flow_data['field'] == 'elementaryExchange'\n",
    "            original_count = flow_data[condition_modify].shape[0]\n",
    "            flow_data.loc[condition_modify, 'flow'] = flow_data.loc[condition_modify, 'flow'].apply(modify_flow)\n",
    "            modified_count = flow_data[condition_modify]['flow'].str.contains('_').sum()  # A simple check to see if the modification was successful\n",
    "            logger.info(f\"{filename}: Modified 'flow' column for 'elementaryExchange' records. {modified_count}/{original_count} records modified.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{filename}: Error modifying 'flow' column for 'elementaryExchange' - {e}\")\n",
    "\n",
    "        # **End of new feature**\n",
    "\n",
    "        # **New feature: Delete duplicate rows where outputGroup is 'N/A'**\n",
    "        # Find rows that are identical except for 'outputGroup'\n",
    "        subset_cols = [col for col in flow_data.columns if col != 'outputGroup']\n",
    "        duplicates = flow_data.duplicated(subset=subset_cols, keep=False)\n",
    "\n",
    "        duplicated_flow_data = flow_data[duplicates]\n",
    "\n",
    "        # Initialize a list to save the indexes to be deleted\n",
    "        indexes_to_drop = []\n",
    "\n",
    "        # Group and find duplicate rows\n",
    "        grouped = duplicated_flow_data.groupby(subset_cols)\n",
    "\n",
    "        for group_keys, group in grouped:\n",
    "            if set(group['outputGroup']) == {'0', 'N/A'}:\n",
    "                # Find the rows where outputGroup is 'N/A' and mark them for deletion\n",
    "                n_a_rows = group[group['outputGroup'] == 'N/A']\n",
    "                indexes_to_drop.extend(n_a_rows.index.tolist())\n",
    "\n",
    "        if indexes_to_drop:\n",
    "            flow_data = flow_data.drop(indexes_to_drop)\n",
    "            logger.info(f\"{filename}: Removed {len(indexes_to_drop)} duplicate rows with outputGroup 'N/A'.\")\n",
    "\n",
    "        # **End of new feature**\n",
    "\n",
    "        # Generate the output file path, use os.path.splitext to ensure the correct extension is replaced\n",
    "        output_filename = f\"{os.path.splitext(filename)[0]}.csv\"\n",
    "        output_path = os.path.join(batch_folder, output_filename)\n",
    "\n",
    "        # Save 'Flow Data' related records to a CSV file\n",
    "        try:\n",
    "            # Even if flow_data is empty, generate a CSV file with a header\n",
    "            flow_data.to_csv(output_path, index=False, encoding='utf-8')\n",
    "            converted_files += 1\n",
    "            logger.info(f\"{filename}: Flow data processed and saved to '{output_path}'\")\n",
    "        except Exception as e:\n",
    "            failed_files += 1\n",
    "            failed_file_names.append(filename)\n",
    "            logger.error(f\"{filename}: Error saving CSV file - {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"{filename}: Error processing file - {e}\")\n",
    "        failed_files += 1\n",
    "        failed_file_names.append(filename)\n",
    "\n",
    "def main():\n",
    "    global total_files, converted_files, failed_files, failed_file_names\n",
    "\n",
    "    # First pass, traverse all files to build a global map\n",
    "    logger.info(\"Starting first pass to build global activity mapping.\")\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".spold\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            logger.info(f\"Building mapping from file {filename}\")\n",
    "            file_mapping = extract_activity_mapping(file_path)\n",
    "            global_activity_mapping.update(file_mapping)\n",
    "\n",
    "    logger.info(f\"Global activity mapping built with {len(global_activity_mapping)} entries.\")\n",
    "\n",
    "    # Output the global mapping dictionary to a CSV file for inspection\n",
    "    mapping_output_path = os.path.join(batch_folder, \"global_activity_mapping.csv\")\n",
    "    try:\n",
    "        with open(mapping_output_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['activity_id', 'shortname', 'activityName'])\n",
    "            for activity_id, (shortname, activityName) in global_activity_mapping.items():\n",
    "                writer.writerow([activity_id, shortname, activityName])\n",
    "        logger.info(f\"Global activity mapping exported to {mapping_output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exporting global activity mapping to CSV: {e}\")\n",
    "\n",
    "    # Second pass, traverse all files to process data\n",
    "    logger.info(\"Starting second pass to process all files.\")\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".spold\"):\n",
    "            total_files += 1\n",
    "            logger.info(f\"Processing file {filename}\")\n",
    "\n",
    "            try:\n",
    "                process_file(filename)\n",
    "            except Exception as e:\n",
    "                failed_files += 1\n",
    "                failed_file_names.append(filename)\n",
    "                logger.error(f\"{filename}: Error processing file - {e}\")\n",
    "                continue\n",
    "\n",
    "    # Generate a list of failed files and save it to a file\n",
    "    failed_files_log_path = os.path.join(batch_folder, \"failed_files.txt\")\n",
    "    try:\n",
    "        with open(failed_files_log_path, 'w', encoding='utf-8') as f:\n",
    "            for fname in failed_file_names:\n",
    "                f.write(f\"{fname}\\n\")\n",
    "        logger.info(f\"Failed file names saved to {failed_files_log_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing failed files log: {e}\")\n",
    "\n",
    "    # Generate a processing summary and save it to summary.txt,\n",
    "    # including statistics on the number of deleted rows and process_name replacements\n",
    "    summary_log_path = os.path.join(batch_folder, \"summary.txt\")\n",
    "    try:\n",
    "        with open(summary_log_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=== Processing Summary ===\\n\")\n",
    "            f.write(f\"Total .spold files found: {total_files}\\n\")\n",
    "            f.write(f\"Successfully converted to CSV: {converted_files}\\n\")\n",
    "            f.write(f\"Failed to convert: {failed_files}\\n\\n\")\n",
    "            if failed_files > 0:\n",
    "                f.write(\"List of failed files:\\n\")\n",
    "                for fname in failed_file_names:\n",
    "                    f.write(f\"- {fname}\\n\")\n",
    "            f.write(\"\\n=== Deleted Rows Per File ===\\n\")\n",
    "            for fname, count in deleted_rows_per_file.items():\n",
    "                f.write(f\"{fname}: {count} rows deleted\\n\")\n",
    "            f.write(\"\\n=== Replaced Process Name Per File ===\\n\")\n",
    "            for fname, flow in replaced_process_name.items():\n",
    "                f.write(f\"{fname}: process_name replaced with flow '{flow}'\\n\")\n",
    "            f.write(\"\\n=== Non-1 Amount Without activityLinkId Per File ===\\n\")\n",
    "            for fname, count in non1_amount_per_file.items():\n",
    "                f.write(f\"{fname}: {count} rows with amount != 1 and no activityLinkId\\n\")\n",
    "            f.write(\"\\n=== Amount=-1 Without activityLinkId Per File ===\\n\")\n",
    "            for fname, count in neg1_amount_per_file.items():\n",
    "                f.write(f\"{fname}: {count} rows with amount=-1 and no activityLinkId\\n\")\n",
    "            f.write(\"\\n=== Files with Unknown Activity Name After Deletion ===\\n\")\n",
    "            for fname in unknown_activity_files:\n",
    "                f.write(f\"{fname}\\n\")\n",
    "        logger.info(f\"Processing summary saved to {summary_log_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing summary log: {e}\")\n",
    "\n",
    "    # Save non1_amount_per_file to CSV\n",
    "    output_non1_path = os.path.join(batch_folder, \"non1_amount_files.csv\")\n",
    "    try:\n",
    "        with open(output_non1_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['Filename', 'Non-1 Amount Count'])\n",
    "            for fname, count in non1_amount_per_file.items():\n",
    "                if count > 0:\n",
    "                    writer.writerow([fname, count])\n",
    "        logger.info(f\"Non-1 amount files saved to {output_non1_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exporting non1 amount files to CSV: {e}\")\n",
    "\n",
    "    # Save neg1_amount_per_file to CSV\n",
    "    output_neg1_path = os.path.join(batch_folder, \"neg1_amount_files.csv\")\n",
    "    try:\n",
    "        with open(output_neg1_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['Filename', 'Amount=-1 Count'])\n",
    "            for fname, count in neg1_amount_per_file.items():\n",
    "                if count > 0:\n",
    "                    writer.writerow([fname, count])\n",
    "        logger.info(f\"Amount=-1 files saved to {output_neg1_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exporting amount=-1 files to CSV: {e}\")\n",
    "\n",
    "    # Tally the results and record them in summary.txt\n",
    "    logger.info(\"=== Processing Summary ===\")\n",
    "    logger.info(f\"Total .spold files found: {total_files}\")\n",
    "    logger.info(f\"Successfully converted to CSV: {converted_files}\")\n",
    "    logger.info(f\"Failed to convert: {failed_files}\")\n",
    "\n",
    "    if failed_files > 0:\n",
    "        logger.info(\"List of failed files:\")\n",
    "        for fname in failed_file_names:\n",
    "            logger.info(f\"- {fname}\")\n",
    "\n",
    "    # Record the number of deleted rows for each file\n",
    "    logger.info(\"=== Deleted Rows Per File ===\")\n",
    "    for fname, count in deleted_rows_per_file.items():\n",
    "        logger.info(f\"{fname}: {count} rows deleted\")\n",
    "\n",
    "    # Record the replaced process_name for each file\n",
    "    logger.info(\"=== Replaced Process Name Per File ===\")\n",
    "    for fname, flow in replaced_process_name.items():\n",
    "        logger.info(f\"{fname}: process_name replaced with flow '{flow}'\")\n",
    "\n",
    "    # Record the number of rows where activityLinkId is missing and amount != 1\n",
    "    logger.info(\"=== Non-1 Amount Without activityLinkId Per File ===\")\n",
    "    for fname, count in non1_amount_per_file.items():\n",
    "        logger.info(f\"{fname}: {count} rows with amount != 1 and no activityLinkId\")\n",
    "\n",
    "    # Record the number of rows where activityLinkId is missing and amount == -1\n",
    "    logger.info(\"=== Amount=-1 Without activityLinkId Per File ===\")\n",
    "    for fname, count in neg1_amount_per_file.items():\n",
    "        logger.info(f\"{fname}: {count} rows with amount=-1 and no activityLinkId\")\n",
    "\n",
    "    # Record files that still have [Unknown Location]Unknown Activity Name\n",
    "    logger.info(\"=== Files with Unknown Activity Name After Deletion ===\")\n",
    "    for fname in unknown_activity_files:\n",
    "        logger.info(f\"{fname}\")\n",
    "\n",
    "    # Increment the batch number for the next run\n",
    "    try:\n",
    "        with open(batch_file_path, \"w\") as f:\n",
    "            f.write(str(batch_number + 1))\n",
    "        logger.info(f\"Batch number incremented to {batch_number + 1}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error updating batch number: {e}\")\n",
    "\n",
    "    logger.info(\"All files processed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ff7e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def compare_directories(dir1, dir2):\n",
    "    \"\"\"\n",
    "    Compare two directories and find files present in dir2 but missing in dir1.\n",
    "    \n",
    "    Args:\n",
    "        dir1 (str): The path to the first directory.\n",
    "        dir2 (str): The path to the second directory.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of files present in dir2 but missing in dir1.\n",
    "    \"\"\"\n",
    "    # List files in both directories\n",
    "    files_dir1 = set(os.listdir(dir1))\n",
    "    files_dir2 = set(os.listdir(dir2))\n",
    "    \n",
    "    # Find files present in dir2 but missing in dir1\n",
    "    missing_files = files_dir2 - files_dir1\n",
    "    return list(missing_files)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the directories to compare\n",
    "    dir1 = r\"C:\\Users\\WasteWang\\LCA\\OUTPUT\\1213_93\"\n",
    "    dir2 = r\"C:\\Users\\WasteWang\\LCA\\OUTPUT\\1211_88\"\n",
    "    \n",
    "    # Compare directories\n",
    "    missing_files = compare_directories(dir1, dir2)\n",
    "    \n",
    "    # Print results\n",
    "    if missing_files:\n",
    "        print(\"Files present in dir2 but missing in dir1:\")\n",
    "        for file in missing_files:\n",
    "            print(file)\n",
    "    else:\n",
    "        print(\"No files are missing in dir1 compared to dir2.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a619f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "def build_lca_matrix(folder_path, output_file):\n",
    "    # A list to record process_names in the order they are processed\n",
    "    process_name_list = []\n",
    "    # Initialize an ordered dictionary to maintain insertion order\n",
    "    data_dict = OrderedDict()\n",
    "    flow_set = set()\n",
    "\n",
    "    # List all CSV file paths\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    total_files = len(csv_files)\n",
    "    print(f\"There are a total of {total_files} CSV files to process.\")\n",
    "\n",
    "    # Used to record files that were not processed successfully\n",
    "    failed_files = []\n",
    "    # Used to detect duplicate process_names\n",
    "    process_name_set = set()\n",
    "    duplicate_process_files = []\n",
    "\n",
    "    for idx, file_name in enumerate(csv_files, 1):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            # Try to read with UTF-8\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            # If UTF-8 fails, try gbk\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding='gbk')\n",
    "            except Exception as e:\n",
    "                print(f\"Error: Could not read file {file_name}. Reason: {e}\")\n",
    "                failed_files.append(file_name)\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error: Could not read file {file_name}. Reason: {e}\")\n",
    "            failed_files.append(file_name)\n",
    "            continue\n",
    "\n",
    "        # Check for required columns\n",
    "        if not {'process_name', 'field', 'flow', 'amount'}.issubset(df.columns):\n",
    "            print(f\"Warning: File {file_name} is missing necessary columns.\")\n",
    "            failed_files.append(file_name)\n",
    "            continue\n",
    "\n",
    "        process_names = df['process_name'].unique()\n",
    "        if len(process_names) < 1:\n",
    "            print(f\"Warning: Could not find a process_name in file {file_name}.\")\n",
    "            failed_files.append(file_name)\n",
    "            continue\n",
    "\n",
    "        process_name = process_names[0]\n",
    "        if len(process_names) > 1:\n",
    "            print(f\"Warning: Multiple process_names exist in file {file_name}. The first one will be used: {process_name}\")\n",
    "\n",
    "        # Check for duplicate process_names\n",
    "        if process_name in process_name_set:\n",
    "            print(f\"Warning: Duplicate process_name '{process_name}' found in file {file_name}, skipping this file.\")\n",
    "            duplicate_process_files.append(file_name)\n",
    "            failed_files.append(file_name)\n",
    "            continue\n",
    "        else:\n",
    "            process_name_set.add(process_name)\n",
    "            # Add the process_name to the list in the order it was processed\n",
    "            process_name_list.append(process_name)\n",
    "\n",
    "        # Filter for the required fields\n",
    "        df_filtered = df[df['field'].isin(['intermediateExchange', 'elementaryExchange'])]\n",
    "\n",
    "        if df_filtered.empty:\n",
    "            print(f\"Warning: No valid rows after filtering in file {file_name}.\")\n",
    "            failed_files.append(file_name)\n",
    "            continue\n",
    "\n",
    "        # Separate intermediate and elementary exchanges\n",
    "        df_intermediate = df_filtered[df_filtered['field'] == 'intermediateExchange']\n",
    "        df_elementary = df_filtered[df_filtered['field'] == 'elementaryExchange']\n",
    "\n",
    "        if df_intermediate.empty and df_elementary.empty:\n",
    "            print(f\"Warning: No intermediateExchange or elementaryExchange rows in file {file_name}.\")\n",
    "            failed_files.append(file_name)\n",
    "            continue\n",
    "\n",
    "        column_added = False\n",
    "\n",
    "        # Process intermediateExchange\n",
    "        for _, row in df_intermediate.iterrows():\n",
    "            flow = row['flow']\n",
    "            amount = row['amount'] if pd.notnull(row['amount']) else 0\n",
    "            if flow not in flow_set:\n",
    "                flow_set.add(flow)\n",
    "                data_dict[flow] = {}\n",
    "            data_dict[flow][process_name] = amount\n",
    "            column_added = True\n",
    "\n",
    "        # Process elementaryExchange\n",
    "        for _, row in df_elementary.iterrows():\n",
    "            flow = row['flow']\n",
    "            amount = row['amount'] if pd.notnull(row['amount']) else 0\n",
    "            if flow not in flow_set:\n",
    "                flow_set.add(flow)\n",
    "                data_dict[flow] = {}\n",
    "            data_dict[flow][process_name] = amount\n",
    "            column_added = True\n",
    "\n",
    "        if not column_added:\n",
    "            failed_files.append(file_name)\n",
    "\n",
    "        if idx % 1000 == 0 or idx == total_files:\n",
    "            print(f\"Processed {idx}/{total_files} files.\")\n",
    "\n",
    "    # Merge data to build the matrix\n",
    "    print(\"Building DataFrame...\")\n",
    "    all_flows = list(data_dict.keys())\n",
    "    matrix_df = pd.DataFrame.from_dict(data_dict, orient='index').fillna(0)\n",
    "    # Sort rows by insertion order\n",
    "    matrix_df = matrix_df.reindex(all_flows)\n",
    "    # Use process_name_list to ensure column order\n",
    "    matrix_df = matrix_df.reindex(columns=process_name_list)\n",
    "    matrix_df.index.name = 'flow'\n",
    "\n",
    "    # Diagonalization process:\n",
    "    # If a column name exists in the row names, move the corresponding row\n",
    "    # to the same index as the column (on the diagonal).\n",
    "    row_order = list(matrix_df.index)\n",
    "    for i, col in enumerate(matrix_df.columns):\n",
    "        if col in row_order:\n",
    "            current_pos = row_order.index(col)\n",
    "            if current_pos != i:\n",
    "                # Swap rows so that (col, col) is on the diagonal\n",
    "                row_order[i], row_order[current_pos] = row_order[current_pos], row_order[i]\n",
    "    matrix_df = matrix_df.reindex(index=row_order)\n",
    "\n",
    "    # Save to CSV\n",
    "    print(f\"Saving matrix to {output_file}...\")\n",
    "    matrix_df.to_csv(output_file, encoding='utf-8')\n",
    "    print(\"Matrix construction complete!\")\n",
    "\n",
    "    # Statistics\n",
    "    print(\"\\n===== Matrix Statistics =====\")\n",
    "    num_rows, num_cols = matrix_df.shape\n",
    "    total_elements = num_rows * num_cols\n",
    "    non_zero = (matrix_df != 0).sum().sum()\n",
    "    non_zero_ratio = non_zero / total_elements if total_elements != 0 else 0\n",
    "    negative = (matrix_df < 0).sum().sum()\n",
    "    negative_ratio = negative / total_elements if total_elements != 0 else 0\n",
    "    negative_over_non_zero = (negative / non_zero) if non_zero != 0 else 0\n",
    "\n",
    "    print(f\"Number of rows: {num_rows}\")\n",
    "    print(f\"Number of columns: {num_cols}\")\n",
    "    print(f\"Number of non-zero values: {non_zero}\")\n",
    "    print(f\"Percentage of non-zero values: {non_zero_ratio:.2%}\")\n",
    "    print(f\"Number of negative values: {negative}\")\n",
    "    print(f\"Percentage of negative values: {negative_ratio:.2%}\")\n",
    "    print(f\"Proportion of negative values among non-zero values: {negative_over_non_zero:.2%}\")\n",
    "    print(\"========================\\n\")\n",
    "\n",
    "    # Count the number of all-zero rows\n",
    "    all_zero_rows = (matrix_df.sum(axis=1) == 0).sum()\n",
    "    print(f\"Number of all-zero rows: {all_zero_rows}\")\n",
    "\n",
    "    # Output the list of files that did not generate a new column\n",
    "    if failed_files:\n",
    "        failed_file_path = os.path.join(os.path.dirname(output_file), \"failed_files.txt\")\n",
    "        with open(failed_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"List of CSV files that did not generate new columns:\\n\")\n",
    "            for file in failed_files:\n",
    "                f.write(f\"{file}\\n\")\n",
    "        print(f\"\\nThe following CSV files did not generate new columns. See '{failed_file_path}' for details:\")\n",
    "        for file in failed_files:\n",
    "            print(f\" - {file}\")\n",
    "    else:\n",
    "        print(\"All CSV files have successfully generated new columns.\")\n",
    "\n",
    "    # Output CSV files with duplicate process_names\n",
    "    if duplicate_process_files:\n",
    "        duplicate_file_path = os.path.join(os.path.dirname(output_file), \"duplicate_process_files.txt\")\n",
    "        with open(duplicate_file_path, \"w\", encoding='utf-8') as f:\n",
    "            f.write(\"List of CSV files with duplicate process_names:\\n\")\n",
    "            for file in duplicate_process_files:\n",
    "                f.write(f\"{file}\\n\")\n",
    "        print(f\"\\nThe following CSV files have duplicate process_names and did not generate new columns. See '{duplicate_file_path}' for details:\")\n",
    "        for file in duplicate_process_files:\n",
    "            print(f\" - {file}\")\n",
    "    else:\n",
    "        print(\"No duplicate process_names were found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = r\"C:\\Users\\WasteWang\\LCA\\OUTPUT\\1213_93\"\n",
    "    output_file = r\"C:\\Users\\WasteWang\\LCA\\OUTPUT\\NEW_3.11_1213_93_LCA_matrix.csv\"\n",
    "    # The filename needs to be changed for each run\n",
    "    build_lca_matrix(folder_path, output_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
